
===============================

Seed:
849
xgbTree_run# 1
eXtreme Gradient Boosting 

No pre-processing
Resampling: Cross-Validated (3 fold, repeated 2 times) 
Summary of sample sizes: 234, 233, 233, 233, 234, 233, ... 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  ROC        Sens       Spec     
  0.1  1           50      0.7643596  1.0000000  0.0000000
  0.1  1          100      0.7614340  0.9983660  0.1047619
  0.1  1          150      0.7538554  0.9918301  0.1507937
  0.1  1          200      0.7462146  0.9836601  0.1722222
  0.1  1          250      0.7432579  0.9787582  0.1722222
  0.1  2           50      0.7408925  0.9934641  0.1722222
  0.1  2          100      0.7343215  0.9771242  0.1611111
  0.1  2          150      0.7302832  0.9705882  0.2166667
  0.1  2          200      0.7292328  0.9673203  0.2277778
  0.1  2          250      0.7281357  0.9673203  0.2388889
  0.1  3           50      0.7417834  0.9820261  0.1944444
  0.1  3          100      0.7295907  0.9705882  0.2166667
  0.1  3          150      0.7280735  0.9575163  0.2166667
  0.1  3          200      0.7269374  0.9558824  0.2277778
  0.1  3          250      0.7268441  0.9558824  0.2277778
  0.1  4           50      0.7380019  0.9705882  0.2055556
  0.1  4          100      0.7367414  0.9624183  0.2277778
  0.1  4          150      0.7387255  0.9558824  0.2277778
  0.1  4          200      0.7400716  0.9558824  0.2277778
  0.1  4          250      0.7401727  0.9558824  0.2388889
  0.1  5           50      0.7252101  0.9656863  0.2285714
  0.1  5          100      0.7295752  0.9624183  0.2055556
  0.1  5          150      0.7294896  0.9624183  0.2055556
  0.1  5          200      0.7284236  0.9640523  0.2055556
  0.1  5          250      0.7307423  0.9624183  0.2166667
  0.3  1           50      0.7538554  0.9901961  0.1722222
  0.3  1          100      0.7412621  0.9754902  0.1833333
  0.3  1          150      0.7368036  0.9656863  0.1595238
  0.3  1          200      0.7350918  0.9607843  0.1706349
  0.3  1          250      0.7320650  0.9542484  0.1825397
  0.3  2           50      0.7304311  0.9673203  0.2055556
  0.3  2          100      0.7301198  0.9607843  0.2166667
  0.3  2          150      0.7312092  0.9607843  0.2166667
  0.3  2          200      0.7351307  0.9607843  0.2166667
  0.3  2          250      0.7353797  0.9624183  0.2285714
  0.3  3           50      0.7329132  0.9689542  0.1936508
  0.3  3          100      0.7353486  0.9591503  0.2285714
  0.3  3          150      0.7310769  0.9575163  0.2285714
  0.3  3          200      0.7320417  0.9575163  0.2396825
  0.3  3          250      0.7320339  0.9558824  0.2396825
  0.3  4           50      0.7232571  0.9656863  0.2166667
  0.3  4          100      0.7252801  0.9656863  0.2166667
  0.3  4          150      0.7253813  0.9607843  0.2055556
  0.3  4          200      0.7248755  0.9575163  0.2166667
  0.3  4          250      0.7246888  0.9558824  0.2277778
  0.3  5           50      0.7314659  0.9607843  0.2055556
  0.3  5          100      0.7281590  0.9526144  0.2388889
  0.3  5          150      0.7307656  0.9526144  0.2396825
  0.3  5          200      0.7284002  0.9558824  0.2396825
  0.3  5          250      0.7306723  0.9607843  0.2285714
  0.5  1           50      0.7392896  0.9738562  0.1952381
  0.5  1          100      0.7347806  0.9591503  0.1936508
  0.5  1          150      0.7329599  0.9444444  0.2047619
  0.5  1          200      0.7254902  0.9411765  0.2047619
  0.5  1          250      0.7257314  0.9379085  0.2047619
  0.5  2           50      0.7268129  0.9575163  0.2285714
  0.5  2          100      0.7332555  0.9591503  0.2404762
  0.5  2          150      0.7315671  0.9575163  0.2404762
  0.5  2          200      0.7324852  0.9575163  0.2515873
  0.5  2          250      0.7300809  0.9542484  0.2515873
  0.5  3           50      0.7398926  0.9509804  0.2269841
  0.5  3          100      0.7378540  0.9509804  0.2388889
  0.5  3          150      0.7367336  0.9509804  0.2619048
  0.5  3          200      0.7356676  0.9509804  0.2619048
  0.5  3          250      0.7348739  0.9526144  0.2500000
  0.5  4           50      0.7286959  0.9656863  0.2166667
  0.5  4          100      0.7287815  0.9624183  0.2388889
  0.5  4          150      0.7293495  0.9640523  0.2277778
  0.5  4          200      0.7303610  0.9591503  0.2277778
  0.5  4          250      0.7291239  0.9575163  0.2388889
  0.5  5           50      0.7240118  0.9509804  0.1944444
  0.5  5          100      0.7254902  0.9493464  0.2277778
  0.5  5          150      0.7255602  0.9460784  0.2277778
  0.5  5          200      0.7271320  0.9444444  0.2507937
  0.5  5          250      0.7259570  0.9428105  0.2507937
  0.7  1           50      0.7349051  0.9656863  0.2063492
  0.7  1          100      0.7287193  0.9477124  0.1936508
  0.7  1          150      0.7261594  0.9346405  0.2055556
  0.7  1          200      0.7248055  0.9346405  0.2055556
  0.7  1          250      0.7262683  0.9346405  0.2063492
  0.7  2           50      0.7322829  0.9575163  0.2293651
  0.7  2          100      0.7347183  0.9509804  0.2404762
  0.7  2          150      0.7343993  0.9526144  0.2404762
  0.7  2          200      0.7325008  0.9477124  0.2404762
  0.7  2          250      0.7318005  0.9493464  0.2404762
  0.7  3           50      0.7199191  0.9575163  0.2507937
  0.7  3          100      0.7246110  0.9542484  0.2507937
  0.7  3          150      0.7272642  0.9542484  0.2507937
  0.7  3          200      0.7276844  0.9558824  0.2507937
  0.7  3          250      0.7291394  0.9575163  0.2619048
  0.7  4           50      0.7332166  0.9624183  0.1714286
  0.7  4          100      0.7316682  0.9607843  0.1825397
  0.7  4          150      0.7291472  0.9607843  0.2047619
  0.7  4          200      0.7326486  0.9607843  0.2055556
  0.7  4          250      0.7296608  0.9640523  0.2055556
  0.7  5           50      0.7192733  0.9460784  0.2166667
  0.7  5          100      0.7190865  0.9477124  0.2166667
  0.7  5          150      0.7190087  0.9493464  0.2285714
  0.7  5          200      0.7181606  0.9477124  0.2396825
  0.7  5          250      0.7171257  0.9477124  0.2396825

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 1

Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using  the largest value.
The final values used for the model were nrounds = 50, max_depth = 1, eta
 = 0.1, gamma = 0, colsample_bytree = 1 and min_child_weight = 1. 

Elapsed time
Time difference of 7.547651 secs

Highest ROC value:
   TrainROC TrainSens TrainSpec  method
1 0.7643596         1         0 xgbTree

Confusion Matrix:

       reference
data    False True
  False   130   18
  True      0    0

F-measure = 0.935251798561151

G-mean = 0

Matthews phi = NaN

Balance = 0.292893218813453

===============================

Seed:
342
xgbTree_run# 2
eXtreme Gradient Boosting 

No pre-processing
Resampling: Cross-Validated (3 fold, repeated 2 times) 
Summary of sample sizes: 233, 234, 233, 234, 233, 233, ... 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  ROC        Sens       Spec      
  0.1  1           50      0.7288827  1.0000000  0.00000000
  0.1  1          100      0.7298786  0.9967320  0.02301587
  0.1  1          150      0.7230859  0.9901961  0.06984127
  0.1  1          200      0.7241441  0.9885621  0.08095238
  0.1  1          250      0.7256847  0.9787582  0.09206349
  0.1  2           50      0.7247549  0.9918301  0.08095238
  0.1  2          100      0.7246965  0.9722222  0.14841270
  0.1  2          150      0.7209384  0.9656863  0.20396825
  0.1  2          200      0.7191254  0.9558824  0.22698413
  0.1  2          250      0.7198257  0.9526144  0.21587302
  0.1  3           50      0.7091659  0.9771242  0.12619048
  0.1  3          100      0.7041550  0.9689542  0.21507937
  0.1  3          150      0.7028789  0.9640523  0.21507937
  0.1  3          200      0.7019374  0.9575163  0.22619048
  0.1  3          250      0.7031279  0.9591503  0.23730159
  0.1  4           50      0.7222611  0.9656863  0.18253968
  0.1  4          100      0.7219265  0.9624183  0.21587302
  0.1  4          150      0.7145036  0.9575163  0.20476190
  0.1  4          200      0.7161064  0.9509804  0.19365079
  0.1  4          250      0.7149549  0.9460784  0.20476190
  0.1  5           50      0.7067460  0.9705882  0.15952381
  0.1  5          100      0.7102163  0.9607843  0.21507937
  0.1  5          150      0.7085356  0.9575163  0.20476190
  0.1  5          200      0.7089325  0.9542484  0.21587302
  0.1  5          250      0.7121538  0.9509804  0.21587302
  0.3  1           50      0.7257703  0.9901961  0.06984127
  0.3  1          100      0.7248755  0.9754902  0.13730159
  0.3  1          150      0.7285325  0.9558824  0.18253968
  0.3  1          200      0.7264550  0.9493464  0.20555556
  0.3  1          250      0.7268830  0.9428105  0.20555556
  0.3  2           50      0.7211018  0.9624183  0.21507937
  0.3  2          100      0.7133909  0.9607843  0.22619048
  0.3  2          150      0.7148459  0.9526144  0.23730159
  0.3  2          200      0.7169390  0.9558824  0.22619048
  0.3  2          250      0.7181528  0.9509804  0.22619048
  0.3  3           50      0.6988873  0.9607843  0.16031746
  0.3  3          100      0.6948646  0.9542484  0.20396825
  0.3  3          150      0.6966853  0.9575163  0.19285714
  0.3  3          200      0.6987473  0.9575163  0.19285714
  0.3  3          250      0.7010349  0.9558824  0.19285714
  0.3  4           50      0.7039293  0.9558824  0.20396825
  0.3  4          100      0.7064970  0.9558824  0.19285714
  0.3  4          150      0.7092048  0.9558824  0.18174603
  0.3  4          200      0.7099440  0.9558824  0.19285714
  0.3  4          250      0.7109555  0.9542484  0.20396825
  0.3  5           50      0.7115235  0.9607843  0.21587302
  0.3  5          100      0.7120993  0.9607843  0.21507937
  0.3  5          150      0.7167834  0.9624183  0.21507937
  0.3  5          200      0.7176937  0.9607843  0.20476190
  0.3  5          250      0.7183318  0.9607843  0.21587302
  0.5  1           50      0.7221289  0.9722222  0.11428571
  0.5  1          100      0.7240741  0.9558824  0.18253968
  0.5  1          150      0.7260271  0.9346405  0.21666667
  0.5  1          200      0.7273498  0.9330065  0.19444444
  0.5  1          250      0.7272642  0.9330065  0.19444444
  0.5  2           50      0.7145425  0.9607843  0.21507937
  0.5  2          100      0.7120448  0.9542484  0.22698413
  0.5  2          150      0.7097650  0.9542484  0.20476190
  0.5  2          200      0.7111734  0.9575163  0.20396825
  0.5  2          250      0.7154217  0.9591503  0.21507937
  0.5  3           50      0.7152194  0.9444444  0.18174603
  0.5  3          100      0.7099518  0.9509804  0.19285714
  0.5  3          150      0.7116636  0.9509804  0.20396825
  0.5  3          200      0.7166433  0.9509804  0.20396825
  0.5  3          250      0.7187364  0.9526144  0.19285714
  0.5  4           50      0.7156552  0.9558824  0.22698413
  0.5  4          100      0.7226268  0.9558824  0.21587302
  0.5  4          150      0.7245331  0.9558824  0.21587302
  0.5  4          200      0.7253968  0.9542484  0.22777778
  0.5  4          250      0.7274199  0.9526144  0.21587302
  0.5  5           50      0.7179661  0.9558824  0.19365079
  0.5  5          100      0.7196234  0.9575163  0.20476190
  0.5  5          150      0.7219110  0.9575163  0.19365079
  0.5  5          200      0.7247043  0.9575163  0.19365079
  0.5  5          250      0.7252568  0.9575163  0.19365079
  0.7  1           50      0.7312636  0.9624183  0.15952381
  0.7  1          100      0.7297930  0.9477124  0.20555556
  0.7  1          150      0.7297930  0.9379085  0.20555556
  0.7  1          200      0.7331699  0.9281046  0.21666667
  0.7  1          250      0.7337846  0.9199346  0.21666667
  0.7  2           50      0.7009648  0.9493464  0.15873016
  0.7  2          100      0.7031746  0.9477124  0.16984127
  0.7  2          150      0.7108855  0.9477124  0.14761905
  0.7  2          200      0.7143946  0.9477124  0.16984127
  0.7  2          250      0.7170635  0.9460784  0.15873016
  0.7  3           50      0.7119592  0.9591503  0.16984127
  0.7  3          100      0.7150016  0.9558824  0.16984127
  0.7  3          150      0.7209462  0.9526144  0.19206349
  0.7  3          200      0.7238796  0.9558824  0.18095238
  0.7  3          250      0.7245798  0.9575163  0.18174603
  0.7  4           50      0.7013228  0.9575163  0.19206349
  0.7  4          100      0.7071662  0.9591503  0.23809524
  0.7  4          150      0.7107610  0.9558824  0.23809524
  0.7  4          200      0.7134454  0.9591503  0.23809524
  0.7  4          250      0.7163243  0.9624183  0.23809524
  0.7  5           50      0.7013617  0.9558824  0.20317460
  0.7  5          100      0.7095861  0.9542484  0.21428571
  0.7  5          150      0.7125506  0.9558824  0.21428571
  0.7  5          200      0.7146203  0.9526144  0.21428571
  0.7  5          250      0.7151572  0.9509804  0.21428571

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 1

Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using  the largest value.
The final values used for the model were nrounds = 250, max_depth = 1, eta
 = 0.7, gamma = 0, colsample_bytree = 1 and min_child_weight = 1. 

Elapsed time
Time difference of 7.423375 secs

Highest ROC value:
   TrainROC TrainSens TrainSpec  method
1 0.7337846 0.9199346 0.2166667 xgbTree

Confusion Matrix:

       reference
data    False True
  False   123   14
  True      7    4

F-measure = 0.921348314606741

G-mean = 0.458537250674806

Matthews phi = 0.209812648977565

Balance = 0.448711654445261
