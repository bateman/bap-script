
===============================

Seed:
849
xgbTree_run# 1
eXtreme Gradient Boosting 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 107613, 107614, 107613, 107614, 107613, 107612, ... 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  ROC        Sens       Spec      
  0.1  1           50      0.8143011  1.0000000  0.00000000
  0.1  1          100      0.8214049  0.9959542  0.07960482
  0.1  1          150      0.8241190  0.9935958  0.10332237
  0.1  1          200      0.8251383  0.9918318  0.11786755
  0.1  1          250      0.8256286  0.9909114  0.12651628
  0.1  2           50      0.8210823  0.9946025  0.09244598
  0.1  2          100      0.8253121  0.9891762  0.13699885
  0.1  2          150      0.8262278  0.9877285  0.14918518
  0.1  2          200      0.8266012  0.9871629  0.15429559
  0.1  2          250      0.8268800  0.9869520  0.15606432
  0.1  3           50      0.8241515  0.9927426  0.10817058
  0.1  3          100      0.8265832  0.9884476  0.14512300
  0.1  3          150      0.8270280  0.9877381  0.14997112
  0.1  3          200      0.8272525  0.9872588  0.15285422
  0.1  3          250      0.8273895  0.9870095  0.15370603
  0.1  4           50      0.8255298  0.9909690  0.12317439
  0.1  4          100      0.8269959  0.9882271  0.14597511
  0.1  4          150      0.8273526  0.9874122  0.15108591
  0.1  4          200      0.8273973  0.9870670  0.15390314
  0.1  4          250      0.8273784  0.9868753  0.15534456
  0.1  5           50      0.8264038  0.9895884  0.13490221
  0.1  5          100      0.8269847  0.9873546  0.15088859
  0.1  5          150      0.8270156  0.9866548  0.15429602
  0.1  5          200      0.8269086  0.9861658  0.15573723
  0.1  5          250      0.8264742  0.9858015  0.15829241
  0.3  1           50      0.8240340  0.9918510  0.11668817
  0.3  1          100      0.8258456  0.9889077  0.14053696
  0.3  1          150      0.8262895  0.9882846  0.14630285
  0.3  1          200      0.8264461  0.9881599  0.14715454
  0.3  1          250      0.8265928  0.9881408  0.14787533
  0.3  2           50      0.8256436  0.9866356  0.15377156
  0.3  2          100      0.8265120  0.9862042  0.15960239
  0.3  2          150      0.8268725  0.9861563  0.16045437
  0.3  2          200      0.8269270  0.9860891  0.15960307
  0.3  2          250      0.8268604  0.9859549  0.15986528
  0.3  3           50      0.8264954  0.9866356  0.15501626
  0.3  3          100      0.8266164  0.9861371  0.15744060
  0.3  3          150      0.8260991  0.9854660  0.15960252
  0.3  3          200      0.8255243  0.9851496  0.16150248
  0.3  3          250      0.8249491  0.9850537  0.16359886
  0.3  4           50      0.8264988  0.9869040  0.15586820
  0.3  4          100      0.8254443  0.9856290  0.16065114
  0.3  4          150      0.8242904  0.9848333  0.16399295
  0.3  4          200      0.8229349  0.9840471  0.16615457
  0.3  4          250      0.8219249  0.9836540  0.16943111
  0.3  5           50      0.8260535  0.9856481  0.15737490
  0.3  5          100      0.8238328  0.9844498  0.16412380
  0.3  5          150      0.8218601  0.9831651  0.16766109
  0.3  5          200      0.8202031  0.9823790  0.17362357
  0.3  5          250      0.8184278  0.9812381  0.17735801
  0.5  1           50      0.8238528  0.9883805  0.14217497
  0.5  1          100      0.8253788  0.9884667  0.14466505
  0.5  1          150      0.8258208  0.9884284  0.14689276
  0.5  1          200      0.8260846  0.9882750  0.14807184
  0.5  1          250      0.8262806  0.9881983  0.14807180
  0.5  2           50      0.8259742  0.9855906  0.16176409
  0.5  2          100      0.8263612  0.9860796  0.16052003
  0.5  2          150      0.8260634  0.9858111  0.15940644
  0.5  2          200      0.8256715  0.9854660  0.16176465
  0.5  2          250      0.8251078  0.9852934  0.16333674
  0.5  3           50      0.8253820  0.9857248  0.15993068
  0.5  3          100      0.8248267  0.9849099  0.16333811
  0.5  3          150      0.8234153  0.9840183  0.16608972
  0.5  3          200      0.8223150  0.9830500  0.16838210
  0.5  3          250      0.8208320  0.9824652  0.16988922
  0.5  4           50      0.8240949  0.9851592  0.16215865
  0.5  4          100      0.8206646  0.9833952  0.16504153
  0.5  4          150      0.8180230  0.9815449  0.17231300
  0.5  4          200      0.8154431  0.9802698  0.17670301
  0.5  4          250      0.8135481  0.9795316  0.18083072
  0.5  5           50      0.8215877  0.9828104  0.16779138
  0.5  5          100      0.8168756  0.9806054  0.17991196
  0.5  5          150      0.8118347  0.9782565  0.18587406
  0.5  5          200      0.8088243  0.9765404  0.19046070
  0.5  5          250      0.8061436  0.9743546  0.19216433
  0.7  1           50      0.8236879  0.9881120  0.14427148
  0.7  1          100      0.8254153  0.9882846  0.14538534
  0.7  1          150      0.8259241  0.9882175  0.14820243
  0.7  1          200      0.8262286  0.9881312  0.14656476
  0.7  1          250      0.8263298  0.9881120  0.14663029
  0.7  2           50      0.8253903  0.9858782  0.15960239
  0.7  2          100      0.8248033  0.9854468  0.16065075
  0.7  2          150      0.8241635  0.9847661  0.16071637
  0.7  2          200      0.8234048  0.9842388  0.16366448
  0.7  2          250      0.8228206  0.9839992  0.16654689
  0.7  3           50      0.8232850  0.9847757  0.16110861
  0.7  3          100      0.8211943  0.9834048  0.16510577
  0.7  3          150      0.8186593  0.9820818  0.17060980
  0.7  3          200      0.8164868  0.9806629  0.17408246
  0.7  3          250      0.8142218  0.9794932  0.17735840
  0.7  4           50      0.8205898  0.9829254  0.16753028
  0.7  4          100      0.8162667  0.9799151  0.17565434
  0.7  4          150      0.8127865  0.9772019  0.18489152
  0.7  4          200      0.8083652  0.9755242  0.18901984
  0.7  4          250      0.8052022  0.9734821  0.19288534
  0.7  5           50      0.8153168  0.9792536  0.18010975
  0.7  5          100      0.8076736  0.9748914  0.19164111
  0.7  5          150      0.8011762  0.9714688  0.19793018
  0.7  5          200      0.7978452  0.9679312  0.20546545
  0.7  5          250      0.7945974  0.9658604  0.20808625

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 1

Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using  the largest value.
The final values used for the model were nrounds = 200, max_depth = 4, eta
 = 0.1, gamma = 0, colsample_bytree = 1 and min_child_weight = 1. 

Elapsed time
Time difference of 41.04569 mins

Highest ROC value:
   TrainROC TrainSens TrainSpec  method
1 0.8273973  0.987067 0.1539031 xgbTree

Confusion Matrix:

       reference
data    False  True
  False 44118  5509
  True    585  1031

F-measure = 0.935397010495071

G-mean = 0.39443916607237

Matthews phi = NA

Balance = 0.404293377078788

===============================

Seed:
342
xgbTree_run# 2
eXtreme Gradient Boosting 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 107612, 107613, 107613, 107613, 107613, 107613, ... 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  ROC        Sens       Spec      
  0.1  1           50      0.8149450  1.0000000  0.00000000
  0.1  1          100      0.8227857  0.9957146  0.08255332
  0.1  1          150      0.8253261  0.9934616  0.10614024
  0.1  1          200      0.8263912  0.9916113  0.12212624
  0.1  1          250      0.8268666  0.9905088  0.13070901
  0.1  2           50      0.8224818  0.9946121  0.09303597
  0.1  2          100      0.8265563  0.9891858  0.14119158
  0.1  2          150      0.8273841  0.9875751  0.15291962
  0.1  2          200      0.8277403  0.9869616  0.15789889
  0.1  2          250      0.8278878  0.9867602  0.16117487
  0.1  3           50      0.8253065  0.9927330  0.11347754
  0.1  3          100      0.8274474  0.9880545  0.15200176
  0.1  3          150      0.8281397  0.9870287  0.15711222
  0.1  3          200      0.8283507  0.9869328  0.15927435
  0.1  3          250      0.8285048  0.9866356  0.16065041
  0.1  4           50      0.8268034  0.9910169  0.12671236
  0.1  4          100      0.8281464  0.9878148  0.15128169
  0.1  4          150      0.8284875  0.9872684  0.15527821
  0.1  4          200      0.8286009  0.9868561  0.15757145
  0.1  4          250      0.8283389  0.9864918  0.15875070
  0.1  5           50      0.8274228  0.9895501  0.13830947
  0.1  5          100      0.8280356  0.9872875  0.15357514
  0.1  5          150      0.8278849  0.9867794  0.15809574
  0.1  5          200      0.8275432  0.9864055  0.16084747
  0.1  5          250      0.8269751  0.9859933  0.16215766
  0.3  1           50      0.8250094  0.9916017  0.12173322
  0.3  1          100      0.8268639  0.9885722  0.14643318
  0.3  1          150      0.8272012  0.9880065  0.15029864
  0.3  1          200      0.8273996  0.9879394  0.15167449
  0.3  1          250      0.8275042  0.9877860  0.15200219
  0.3  2           50      0.8268505  0.9867986  0.15868508
  0.3  2          100      0.8277675  0.9861083  0.16504011
  0.3  2          150      0.8278970  0.9860891  0.16641604
  0.3  2          200      0.8280900  0.9860220  0.16752981
  0.3  2          250      0.8278783  0.9858303  0.16661251
  0.3  3           50      0.8276291  0.9866260  0.15848914
  0.3  3          100      0.8274865  0.9859549  0.16333743
  0.3  3          150      0.8271761  0.9854468  0.16438545
  0.3  3          200      0.8267333  0.9850921  0.16523731
  0.3  3          250      0.8260436  0.9848332  0.16530292
  0.3  4           50      0.8273823  0.9868944  0.15822611
  0.3  4          100      0.8261667  0.9857632  0.16202608
  0.3  4          150      0.8248634  0.9847853  0.16438472
  0.3  4          200      0.8233735  0.9841142  0.16641579
  0.3  4          250      0.8219437  0.9834335  0.17047831
  0.3  5           50      0.8268792  0.9860700  0.16169903
  0.3  5          100      0.8247040  0.9845456  0.16746484
  0.3  5          150      0.8224209  0.9834144  0.16969237
  0.3  5          200      0.8201271  0.9824077  0.17355761
  0.3  5          250      0.8182525  0.9812764  0.17585068
  0.5  1           50      0.8248024  0.9877573  0.14885740
  0.5  1          100      0.8260923  0.9877669  0.15088795
  0.5  1          150      0.8265850  0.9876998  0.15383653
  0.5  1          200      0.8268923  0.9877956  0.15409879
  0.5  1          250      0.8270347  0.9878723  0.15429521
  0.5  2           50      0.8265630  0.9857344  0.16556414
  0.5  2          100      0.8269859  0.9862329  0.16674361
  0.5  2          150      0.8266852  0.9860029  0.16641583
  0.5  2          200      0.8263395  0.9856577  0.16621950
  0.5  2          250      0.8257440  0.9852263  0.16720186
  0.5  3           50      0.8265227  0.9855714  0.16392660
  0.5  3          100      0.8257965  0.9849387  0.16680927
  0.5  3          150      0.8243795  0.9841909  0.16772675
  0.5  3          200      0.8228351  0.9831555  0.17001964
  0.5  3          250      0.8215762  0.9826761  0.17290235
  0.5  4           50      0.8254151  0.9848907  0.16674344
  0.5  4          100      0.8222953  0.9829446  0.17074005
  0.5  4          150      0.8195288  0.9811901  0.17702959
  0.5  4          200      0.8168475  0.9797425  0.18174665
  0.5  4          250      0.8139286  0.9783236  0.18423622
  0.5  5           50      0.8226907  0.9835294  0.16812019
  0.5  5          100      0.8176593  0.9804999  0.17748822
  0.5  5          150      0.8135287  0.9777867  0.18436745
  0.5  5          200      0.8097194  0.9762432  0.18843032
  0.5  5          250      0.8063079  0.9749202  0.19452318
  0.7  1           50      0.8255200  0.9879490  0.14852978
  0.7  1          100      0.8265310  0.9878915  0.15154334
  0.7  1          150      0.8270079  0.9877669  0.15305021
  0.7  1          200      0.8273117  0.9876710  0.15383653
  0.7  1          250      0.8274940  0.9877765  0.15436061
  0.7  2           50      0.8252798  0.9848812  0.16726799
  0.7  2          100      0.8252372  0.9851400  0.16648261
  0.7  2          150      0.8247516  0.9845552  0.16759573
  0.7  2          200      0.8242185  0.9842868  0.16890609
  0.7  2          250      0.8234558  0.9842101  0.17047870
  0.7  3           50      0.8246059  0.9847661  0.16464761
  0.7  3          100      0.8222739  0.9829350  0.16838167
  0.7  3          150      0.8198444  0.9818229  0.17467172
  0.7  3          200      0.8176066  0.9807491  0.18050281
  0.7  3          250      0.8158523  0.9795507  0.18122399
  0.7  4           50      0.8217931  0.9831267  0.16877460
  0.7  4          100      0.8157908  0.9800972  0.17820987
  0.7  4          150      0.8108822  0.9776429  0.18338560
  0.7  4          200      0.8080712  0.9752557  0.18947855
  0.7  4          250      0.8050873  0.9737122  0.19596520
  0.7  5           50      0.8165716  0.9805478  0.17794655
  0.7  5          100      0.8086448  0.9754667  0.18600602
  0.7  5          150      0.8023468  0.9714593  0.19675041
  0.7  5          200      0.7976732  0.9681805  0.20454729
  0.7  5          250      0.7946090  0.9651893  0.20926516

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 1

Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using  the largest value.
The final values used for the model were nrounds = 200, max_depth = 4, eta
 = 0.1, gamma = 0, colsample_bytree = 1 and min_child_weight = 1. 

Elapsed time
Time difference of 41.07789 mins

Highest ROC value:
   TrainROC TrainSens TrainSpec  method
1 0.8286009 0.9868561 0.1575714 xgbTree

Confusion Matrix:

       reference
data    False  True
  False 44104  5548
  True    599   992

F-measure = 0.934852419055694

G-mean = 0.38684556027045

Matthews phi = NA

Balance = 0.400073733841855

===============================

Seed:
112
xgbTree_run# 3
eXtreme Gradient Boosting 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 107613, 107613, 107614, 107612, 107614, 107613, ... 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  ROC        Sens       Spec      
  0.1  1           50      0.8142644  1.0000000  0.00000000
  0.1  1          100      0.8215932  0.9958967  0.08163511
  0.1  1          150      0.8239955  0.9937013  0.10325620
  0.1  1          200      0.8250078  0.9918989  0.11714620
  0.1  1          250      0.8255578  0.9907964  0.12605688
  0.1  2           50      0.8213955  0.9947654  0.09093838
  0.1  2          100      0.8253513  0.9897035  0.13575398
  0.1  2          150      0.8262398  0.9881695  0.14767818
  0.1  2          200      0.8265801  0.9875176  0.15200244
  0.1  2          250      0.8268708  0.9872300  0.15455750
  0.1  3           50      0.8242967  0.9928576  0.10744965
  0.1  3          100      0.8265870  0.9885913  0.14551571
  0.1  3          150      0.8270550  0.9876710  0.15147811
  0.1  3          200      0.8272494  0.9873354  0.15383692
  0.1  3          250      0.8273685  0.9870478  0.15508183
  0.1  4           50      0.8256151  0.9912086  0.12330523
  0.1  4          100      0.8269741  0.9881216  0.14597464
  0.1  4          150      0.8271347  0.9874217  0.15082268
  0.1  4          200      0.8271051  0.9870766  0.15311557
  0.1  4          250      0.8269585  0.9867986  0.15488460
  0.1  5           50      0.8264676  0.9896555  0.13522944
  0.1  5          100      0.8271734  0.9874984  0.15049536
  0.1  5          150      0.8270327  0.9866931  0.15331237
  0.1  5          200      0.8267466  0.9863288  0.15567114
  0.1  5          250      0.8263599  0.9859453  0.15783284
  0.3  1           50      0.8233445  0.9922440  0.11295321
  0.3  1          100      0.8253944  0.9892049  0.13745744
  0.3  1          150      0.8258311  0.9885242  0.14328831
  0.3  1          200      0.8260550  0.9882558  0.14545048
  0.3  1          250      0.8261730  0.9883133  0.14636761
  0.3  2           50      0.8254432  0.9867698  0.15455763
  0.3  2          100      0.8265147  0.9858207  0.16163337
  0.3  2          150      0.8267007  0.9862425  0.15901321
  0.3  2          200      0.8268004  0.9862329  0.15934061
  0.3  2          250      0.8266811  0.9862042  0.15940609
  0.3  3           50      0.8263347  0.9864918  0.15711290
  0.3  3          100      0.8265293  0.9858303  0.16084726
  0.3  3          150      0.8262028  0.9856673  0.16078190
  0.3  3          200      0.8257542  0.9854084  0.16222285
  0.3  3          250      0.8249241  0.9850441  0.16274679
  0.3  4           50      0.8260500  0.9867123  0.15580181
  0.3  4          100      0.8254687  0.9858590  0.16130576
  0.3  4          150      0.8241771  0.9851400  0.16359908
  0.3  4          200      0.8228850  0.9846894  0.16471280
  0.3  4          250      0.8217256  0.9841046  0.16759526
  0.3  5           50      0.8258429  0.9860795  0.15934074
  0.3  5          100      0.8238357  0.9846990  0.16595711
  0.3  5          150      0.8217183  0.9830117  0.17113263
  0.3  5          200      0.8197833  0.9820338  0.17434390
  0.3  5          250      0.8177112  0.9810655  0.17729184
  0.5  1           50      0.8234350  0.9880353  0.14354974
  0.5  1          100      0.8250717  0.9884188  0.14302593
  0.5  1          150      0.8255770  0.9882462  0.14479504
  0.5  1          200      0.8258545  0.9882079  0.14630225
  0.5  1          250      0.8260179  0.9881216  0.14689203
  0.5  2           50      0.8254891  0.9858111  0.15927430
  0.5  2          100      0.8262113  0.9865014  0.15953711
  0.5  2          150      0.8257689  0.9862713  0.15966809
  0.5  2          200      0.8254403  0.9862234  0.16130580
  0.5  2          250      0.8250298  0.9857152  0.16353312
  0.5  3           50      0.8252540  0.9856960  0.16104394
  0.5  3          100      0.8241231  0.9849291  0.16379533
  0.5  3          150      0.8228966  0.9845839  0.16517053
  0.5  3          200      0.8211782  0.9837978  0.16818426
  0.5  3          250      0.8196834  0.9828295  0.16949466
  0.5  4           50      0.8241296  0.9853509  0.16320667
  0.5  4          100      0.8210587  0.9836828  0.16857852
  0.5  4          150      0.8181932  0.9819571  0.17336076
  0.5  4          200      0.8152645  0.9807299  0.17820828
  0.5  4          250      0.8124037  0.9792248  0.18240135
  0.5  5           50      0.8217985  0.9836061  0.16707114
  0.5  5          100      0.8162313  0.9806532  0.17467074
  0.5  5          150      0.8120282  0.9788892  0.18017429
  0.5  5          200      0.8079132  0.9767609  0.18718523
  0.5  5          250      0.8048996  0.9752174  0.19314742
  0.7  1           50      0.8236183  0.9885434  0.14361540
  0.7  1          100      0.8251622  0.9885243  0.14636757
  0.7  1          150      0.8255432  0.9881408  0.14852953
  0.7  1          200      0.8258316  0.9880832  0.14774350
  0.7  1          250      0.8260310  0.9879970  0.14800575
  0.7  2           50      0.8250658  0.9862425  0.15979885
  0.7  2          100      0.8248932  0.9861179  0.15986451
  0.7  2          150      0.8243092  0.9856097  0.16182988
  0.7  2          200      0.8236403  0.9849099  0.16543360
  0.7  2          250      0.8229005  0.9844497  0.16523675
  0.7  3           50      0.8236317  0.9851592  0.16176469
  0.7  3          100      0.8208830  0.9838266  0.16563002
  0.7  3          150      0.8183369  0.9824652  0.16811976
  0.7  3          200      0.8167836  0.9813244  0.17303328
  0.7  3          250      0.8147271  0.9803944  0.17866713
  0.7  4           50      0.8208210  0.9829062  0.16949440
  0.7  4          100      0.8156241  0.9797233  0.17925712
  0.7  4          150      0.8115379  0.9773936  0.18384333
  0.7  4          200      0.8078853  0.9752749  0.19111575
  0.7  4          250      0.8043109  0.9732328  0.19576792
  0.7  5           50      0.8160459  0.9799534  0.17657079
  0.7  5          100      0.8080016  0.9749489  0.18757799
  0.7  5          150      0.8010288  0.9714113  0.19589842
  0.7  5          200      0.7967091  0.9684873  0.20310638
  0.7  5          250      0.7923637  0.9659275  0.20893785

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 1

Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using  the largest value.
The final values used for the model were nrounds = 250, max_depth = 3, eta
 = 0.1, gamma = 0, colsample_bytree = 1 and min_child_weight = 1. 

Elapsed time
Time difference of 41.04856 mins

Highest ROC value:
   TrainROC TrainSens TrainSpec  method
1 0.8273685 0.9870478 0.1550818 xgbTree

Confusion Matrix:

       reference
data    False  True
  False 44114  5526
  True    589  1014

F-measure = 0.935183320437128

G-mean = 0.391155992528627

Matthews phi = NA

Balance = 0.402454569853325

===============================

Seed:
345
xgbTree_run# 4
eXtreme Gradient Boosting 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 107612, 107614, 107613, 107614, 107613, 107613, ... 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  ROC        Sens       Spec      
  0.1  1           50      0.8133803  1.0000000  0.00000000
  0.1  1          100      0.8208102  0.9958776  0.07927617
  0.1  1          150      0.8234145  0.9938739  0.10312436
  0.1  1          200      0.8244719  0.9920619  0.11799638
  0.1  1          250      0.8248346  0.9908539  0.12625184
  0.1  2           50      0.8204093  0.9943628  0.09434504
  0.1  2          100      0.8246029  0.9890803  0.13896216
  0.1  2          150      0.8253495  0.9874026  0.15121406
  0.1  2          200      0.8257036  0.9871437  0.15448995
  0.1  2          250      0.8259860  0.9869041  0.15632430
  0.1  3           50      0.8235614  0.9920331  0.11465598
  0.1  3          100      0.8257596  0.9877956  0.14872457
  0.1  3          150      0.8262063  0.9872012  0.15383465
  0.1  3          200      0.8265116  0.9868178  0.15521045
  0.1  3          250      0.8265913  0.9866644  0.15638979
  0.1  4           50      0.8247093  0.9905184  0.12893865
  0.1  4          100      0.8260263  0.9878148  0.14839675
  0.1  4          150      0.8262959  0.9874122  0.15285190
  0.1  4          200      0.8263149  0.9870670  0.15488284
  0.1  4          250      0.8262289  0.9867411  0.15501411
  0.1  5           50      0.8253669  0.9891954  0.13896229
  0.1  5          100      0.8261266  0.9872971  0.15232804
  0.1  5          150      0.8261032  0.9869903  0.15599712
  0.1  5          200      0.8258245  0.9866164  0.15802793
  0.1  5          250      0.8256686  0.9863480  0.15815921
  0.3  1           50      0.8234095  0.9918126  0.11753775
  0.3  1          100      0.8250023  0.9887831  0.14066553
  0.3  1          150      0.8253520  0.9884572  0.14420334
  0.3  1          200      0.8255322  0.9883325  0.14570999
  0.3  1          250      0.8256666  0.9883038  0.14623423
  0.3  2           50      0.8248246  0.9865493  0.15619268
  0.3  2          100      0.8257872  0.9861562  0.15979645
  0.3  2          150      0.8261211  0.9860700  0.15973130
  0.3  2          200      0.8262027  0.9859262  0.16136949
  0.3  2          250      0.8261271  0.9857344  0.16117259
  0.3  3           50      0.8254544  0.9866452  0.15573513
  0.3  3          100      0.8258828  0.9862809  0.15789738
  0.3  3          150      0.8254140  0.9861275  0.15940399
  0.3  3          200      0.8250909  0.9857440  0.15946965
  0.3  3          250      0.8244461  0.9853318  0.15933872
  0.3  4           50      0.8256340  0.9869520  0.15475173
  0.3  4          100      0.8248009  0.9858974  0.16012517
  0.3  4          150      0.8236317  0.9852455  0.16202467
  0.3  4          200      0.8225569  0.9842868  0.16418692
  0.3  4          250      0.8213817  0.9839033  0.16746265
  0.3  5           50      0.8249245  0.9861179  0.15946931
  0.3  5          100      0.8224715  0.9847374  0.16529975
  0.3  5          150      0.8207031  0.9835198  0.16936158
  0.3  5          200      0.8190890  0.9822064  0.17217924
  0.3  5          250      0.8171930  0.9813915  0.17656920
  0.5  1           50      0.8235729  0.9888598  0.13843851
  0.5  1          100      0.8248668  0.9889749  0.14086242
  0.5  1          150      0.8253616  0.9888406  0.14387611
  0.5  1          200      0.8255562  0.9886777  0.14512072
  0.5  1          250      0.8257067  0.9886201  0.14531732
  0.5  2           50      0.8245080  0.9855619  0.16261461
  0.5  2          100      0.8251349  0.9861275  0.15829057
  0.5  2          150      0.8251125  0.9855810  0.16077980
  0.5  2          200      0.8248957  0.9856290  0.15953522
  0.5  2          250      0.8245675  0.9856002  0.15992845
  0.5  3           50      0.8247846  0.9855619  0.15874877
  0.5  3          100      0.8235542  0.9851017  0.16150115
  0.5  3          150      0.8223587  0.9844593  0.16490763
  0.5  3          200      0.8211262  0.9836636  0.16674258
  0.5  3          250      0.8200605  0.9827912  0.16883897
  0.5  4           50      0.8234201  0.9851208  0.16235228
  0.5  4          100      0.8207175  0.9838170  0.16733189
  0.5  4          150      0.8183091  0.9825707  0.17126296
  0.5  4          200      0.8162400  0.9813915  0.17748740
  0.5  4          250      0.8134258  0.9799630  0.18023892
  0.5  5           50      0.8216171  0.9838554  0.16824850
  0.5  5          100      0.8173311  0.9814873  0.17519370
  0.5  5          150      0.8133585  0.9791864  0.18089316
  0.5  5          200      0.8090275  0.9768280  0.18678981
  0.5  5          250      0.8060098  0.9753132  0.19334174
  0.7  1           50      0.8223655  0.9887544  0.14263133
  0.7  1          100      0.8240693  0.9885243  0.14413823
  0.7  1          150      0.8249025  0.9885722  0.14492456
  0.7  1          200      0.8252164  0.9886585  0.14538298
  0.7  1          250      0.8253613  0.9884763  0.14590688
  0.7  2           50      0.8240012  0.9858782  0.16005964
  0.7  2          100      0.8241906  0.9855523  0.16104235
  0.7  2          150      0.8234404  0.9851113  0.16346626
  0.7  2          200      0.8226756  0.9846990  0.16366246
  0.7  2          250      0.8220699  0.9845265  0.16294171
  0.7  3           50      0.8231595  0.9852455  0.16484167
  0.7  3          100      0.8204965  0.9837403  0.16982094
  0.7  3          150      0.8178706  0.9826474  0.17145874
  0.7  3          200      0.8155112  0.9814298  0.17440694
  0.7  3          250      0.8136461  0.9798000  0.17801019
  0.7  4           50      0.8195540  0.9824940  0.17159019
  0.7  4          100      0.8147380  0.9806245  0.18030325
  0.7  4          150      0.8112795  0.9782181  0.18449662
  0.7  4          200      0.8079269  0.9763487  0.19118004
  0.7  4          250      0.8049916  0.9739903  0.19511072
  0.7  5           50      0.8163515  0.9802123  0.17899380
  0.7  5          100      0.8079675  0.9757543  0.18823235
  0.7  5          150      0.8027587  0.9726960  0.19792842
  0.7  5          200      0.7982012  0.9697048  0.20277650
  0.7  5          250      0.7942646  0.9669246  0.21024585

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 1

Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using  the largest value.
The final values used for the model were nrounds = 250, max_depth = 3, eta
 = 0.1, gamma = 0, colsample_bytree = 1 and min_child_weight = 1. 

Elapsed time
Time difference of 41.55623 mins

Highest ROC value:
   TrainROC TrainSens TrainSpec  method
1 0.8265913 0.9866644 0.1563898 xgbTree

Confusion Matrix:

       reference
data    False  True
  False 44095  5495
  True    608  1045

F-measure = 0.935276213504714

G-mean = 0.397004664860036

Matthews phi = NA

Balance = 0.405801101185498

===============================

Seed:
355
xgbTree_run# 5
eXtreme Gradient Boosting 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 107613, 107612, 107613, 107613, 107613, 107614, ... 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  ROC        Sens       Spec      
  0.1  1           50      0.8129966  1.0000000  0.00000000
  0.1  1          100      0.8204400  0.9955804  0.08124120
  0.1  1          150      0.8231515  0.9936533  0.10377976
  0.1  1          200      0.8241531  0.9918510  0.11996295
  0.1  1          250      0.8246268  0.9906430  0.12861125
  0.1  2           50      0.8203680  0.9941327  0.09421359
  0.1  2          100      0.8245005  0.9889173  0.14230389
  0.1  2          150      0.8253403  0.9872012  0.15363844
  0.1  2          200      0.8255780  0.9866068  0.15861792
  0.1  2          250      0.8257688  0.9865301  0.15973195
  0.1  3           50      0.8233861  0.9921673  0.11609740
  0.1  3          100      0.8258123  0.9874121  0.15291816
  0.1  3          150      0.8262592  0.9867410  0.15802853
  0.1  3          200      0.8265059  0.9867123  0.15966654
  0.1  3          250      0.8266938  0.9864438  0.16078074
  0.1  4           50      0.8246647  0.9904992  0.13044539
  0.1  4          100      0.8262487  0.9876135  0.15088739
  0.1  4          150      0.8264844  0.9871629  0.15383576
  0.1  4          200      0.8265502  0.9866644  0.15671869
  0.1  4          250      0.8264891  0.9863480  0.15802879
  0.1  5           50      0.8255708  0.9890611  0.14073201
  0.1  5          100      0.8264752  0.9871437  0.15593228
  0.1  5          150      0.8262860  0.9866068  0.15979752
  0.1  5          200      0.8261100  0.9861083  0.16182855
  0.1  5          250      0.8258000  0.9859549  0.16228739
  0.3  1           50      0.8227806  0.9917647  0.11688356
  0.3  1          100      0.8245761  0.9885434  0.14426977
  0.3  1          150      0.8249680  0.9879107  0.14970775
  0.3  1          200      0.8252216  0.9878148  0.15062497
  0.3  1          250      0.8253425  0.9876422  0.15069054
  0.3  2           50      0.8245582  0.9863576  0.15815934
  0.3  2          100      0.8254647  0.9857632  0.16294248
  0.3  2          150      0.8256157  0.9858494  0.16248420
  0.3  2          200      0.8257930  0.9858398  0.16268036
  0.3  2          250      0.8256942  0.9859357  0.16163221
  0.3  3           50      0.8255414  0.9860987  0.16117264
  0.3  3          100      0.8261938  0.9859549  0.16241819
  0.3  3          150      0.8257684  0.9853988  0.16530112
  0.3  3          200      0.8252260  0.9851400  0.16536717
  0.3  3          250      0.8244565  0.9846127  0.16589124
  0.3  4           50      0.8257930  0.9862042  0.15992862
  0.3  4          100      0.8247662  0.9850633  0.16340154
  0.3  4          150      0.8237150  0.9844401  0.16687412
  0.3  4          200      0.8225145  0.9837019  0.16962581
  0.3  4          250      0.8210955  0.9829349  0.17073953
  0.3  5           50      0.8252089  0.9855522  0.16340111
  0.3  5          100      0.8233469  0.9836923  0.16759448
  0.3  5          150      0.8213845  0.9827049  0.17218112
  0.3  5          200      0.8192515  0.9820050  0.17427755
  0.3  5          250      0.8169514  0.9809025  0.17643921
  0.5  1           50      0.8226672  0.9874505  0.14964179
  0.5  1          100      0.8242938  0.9875464  0.15101789
  0.5  1          150      0.8248106  0.9874697  0.15160793
  0.5  1          200      0.8250533  0.9874505  0.15272152
  0.5  1          250      0.8251692  0.9874505  0.15344219
  0.5  2           50      0.8245881  0.9859262  0.16169834
  0.5  2          100      0.8248837  0.9859166  0.16091206
  0.5  2          150      0.8247103  0.9859166  0.16196021
  0.5  2          200      0.8244155  0.9855523  0.16241884
  0.5  2          250      0.8242006  0.9850537  0.16268075
  0.5  3           50      0.8245576  0.9854084  0.16490849
  0.5  3          100      0.8236694  0.9842771  0.16811912
  0.5  3          150      0.8222444  0.9836157  0.16851191
  0.5  3          200      0.8209689  0.9828487  0.17073919
  0.5  3          250      0.8197573  0.9822351  0.17191887
  0.5  4           50      0.8229876  0.9841621  0.16556397
  0.5  4          100      0.8207816  0.9826186  0.16916709
  0.5  4          150      0.8175539  0.9808833  0.17119808
  0.5  4          200      0.8148351  0.9794644  0.17643960
  0.5  4          250      0.8121075  0.9782373  0.18069751
  0.5  5           50      0.8211302  0.9830500  0.17368773
  0.5  5          100      0.8157427  0.9802410  0.18017404
  0.5  5          150      0.8113517  0.9780360  0.18731543
  0.5  5          200      0.8078429  0.9763390  0.19157373
  0.5  5          250      0.8048142  0.9741149  0.19537405
  0.7  1           50      0.8228046  0.9874601  0.15003571
  0.7  1          100      0.8241307  0.9877764  0.14931521
  0.7  1          150      0.8247026  0.9876998  0.14997013
  0.7  1          200      0.8250251  0.9877093  0.15049421
  0.7  1          250      0.8251514  0.9875847  0.15154227
  0.7  2           50      0.8237951  0.9856960  0.16326988
  0.7  2          100      0.8237801  0.9849291  0.16490841
  0.7  2          150      0.8227208  0.9846798  0.16615319
  0.7  2          200      0.8223848  0.9843443  0.16707092
  0.7  2          250      0.8217779  0.9835869  0.16956062
  0.7  3           50      0.8219610  0.9840662  0.16759405
  0.7  3          100      0.8200321  0.9829925  0.16949458
  0.7  3          150      0.8175372  0.9816503  0.17231094
  0.7  3          200      0.8159620  0.9805574  0.17584875
  0.7  3          250      0.8137744  0.9793302  0.18030471
  0.7  4           50      0.8199921  0.9822735  0.17506319
  0.7  4          100      0.8148986  0.9796945  0.17919090
  0.7  4          150      0.8104499  0.9770677  0.18646383
  0.7  4          200      0.8071787  0.9750928  0.19078775
  0.7  4          250      0.8048507  0.9738273  0.19216394
  0.7  5           50      0.8142487  0.9798288  0.17787973
  0.7  5          100      0.8063033  0.9753804  0.18737994
  0.7  5          150      0.8000831  0.9713730  0.19596323
  0.7  5          200      0.7954996  0.9686790  0.20199189
  0.7  5          250      0.7920193  0.9661096  0.20906775

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 1

Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using  the largest value.
The final values used for the model were nrounds = 250, max_depth = 3, eta
 = 0.1, gamma = 0, colsample_bytree = 1 and min_child_weight = 1. 

Elapsed time
Time difference of 41.12229 mins

Highest ROC value:
   TrainROC TrainSens TrainSpec  method
1 0.8266938 0.9864438 0.1607807 xgbTree

Confusion Matrix:

       reference
data    False  True
  False 44058  5473
  True    645  1067

F-measure = 0.935076511662457

G-mean = 0.400993552861595

Matthews phi = NA

Balance = 0.408169635024415

===============================

Seed:
977
xgbTree_run# 6
eXtreme Gradient Boosting 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 107613, 107613, 107613, 107612, 107613, 107613, ... 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  ROC        Sens       Spec      
  0.1  1           50      0.8136744  1.0000000  0.00000000
  0.1  1          100      0.8211969  0.9959159  0.07953851
  0.1  1          150      0.8237841  0.9941231  0.09834212
  0.1  1          200      0.8248798  0.9923975  0.11288729
  0.1  1          250      0.8253835  0.9910553  0.12310838
  0.1  2           50      0.8207052  0.9952544  0.08536899
  0.1  2          100      0.8249742  0.9899336  0.13339414
  0.1  2          150      0.8258869  0.9880257  0.14571209
  0.1  2          200      0.8261173  0.9874697  0.14951223
  0.1  2          250      0.8263782  0.9871054  0.15115007
  0.1  3           50      0.8236383  0.9933370  0.10378040
  0.1  3          100      0.8260639  0.9887256  0.14237015
  0.1  3          150      0.8266005  0.9878819  0.14767724
  0.1  3          200      0.8268321  0.9874218  0.14924998
  0.1  3          250      0.8269742  0.9872396  0.15010124
  0.1  4           50      0.8251318  0.9912087  0.12186330
  0.1  4          100      0.8265671  0.9883421  0.14368055
  0.1  4          150      0.8269216  0.9876998  0.14695683
  0.1  4          200      0.8270333  0.9873259  0.14807073
  0.1  4          250      0.8268969  0.9871246  0.15003674
  0.1  5           50      0.8259622  0.9902883  0.13070815
  0.1  5          100      0.8266218  0.9881312  0.14387748
  0.1  5          150      0.8264347  0.9874026  0.14793962
  0.1  5          200      0.8261432  0.9869424  0.14964295
  0.1  5          250      0.8257796  0.9865589  0.15082229
  0.3  1           50      0.8234633  0.9926371  0.10895656
  0.3  1          100      0.8254538  0.9892337  0.13581895
  0.3  1          150      0.8257235  0.9885722  0.14138779
  0.3  1          200      0.8259466  0.9883805  0.14243658
  0.3  1          250      0.8260633  0.9882942  0.14276423
  0.3  2           50      0.8251604  0.9871821  0.15075702
  0.3  2          100      0.8259863  0.9865493  0.15462256
  0.3  2          150      0.8263101  0.9865110  0.15560527
  0.3  2          200      0.8262064  0.9864726  0.15547399
  0.3  2          250      0.8263131  0.9863480  0.15567054
  0.3  3           50      0.8258517  0.9872396  0.15101918
  0.3  3          100      0.8257149  0.9864918  0.15573607
  0.3  3          150      0.8254754  0.9860029  0.15685035
  0.3  3          200      0.8250716  0.9857152  0.15888129
  0.3  3          250      0.8244328  0.9854276  0.15829177
  0.3  4           50      0.8255297  0.9870766  0.14715381
  0.3  4          100      0.8244693  0.9857249  0.15246086
  0.3  4          150      0.8230861  0.9849291  0.15396760
  0.3  4          200      0.8218651  0.9841526  0.15750527
  0.3  4          250      0.8207601  0.9837499  0.16051874
  0.3  5           50      0.8254503  0.9862426  0.15272247
  0.3  5          100      0.8234881  0.9845456  0.15757050
  0.3  5          150      0.8212246  0.9836828  0.16300844
  0.3  5          200      0.8195762  0.9821105  0.16602230
  0.3  5          250      0.8176144  0.9812573  0.16890514
  0.5  1           50      0.8235832  0.9883996  0.14033934
  0.5  1          100      0.8249700  0.9884284  0.14355017
  0.5  1          150      0.8255489  0.9883997  0.14348490
  0.5  1          200      0.8257384  0.9882846  0.14387813
  0.5  1          250      0.8259141  0.9882942  0.14400885
  0.5  2           50      0.8250180  0.9863959  0.15364050
  0.5  2          100      0.8252594  0.9864822  0.15252613
  0.5  2          150      0.8247716  0.9860891  0.15350922
  0.5  2          200      0.8247913  0.9859453  0.15521290
  0.5  2          250      0.8242684  0.9857248  0.15599879
  0.5  3           50      0.8245314  0.9862138  0.15462308
  0.5  3          100      0.8239203  0.9852359  0.15586790
  0.5  3          150      0.8222394  0.9844881  0.15822628
  0.5  3          200      0.8207218  0.9837499  0.15986399
  0.5  3          250      0.8195823  0.9832034  0.16143665
  0.5  4           50      0.8244302  0.9854660  0.15770199
  0.5  4          100      0.8216985  0.9838074  0.16340124
  0.5  4          150      0.8193236  0.9825419  0.16674254
  0.5  4          200      0.8165398  0.9807875  0.16916709
  0.5  4          250      0.8143664  0.9794932  0.17322966
  0.5  5           50      0.8211675  0.9839033  0.16110878
  0.5  5          100      0.8162046  0.9812285  0.16923254
  0.5  5          150      0.8117119  0.9786113  0.17643951
  0.5  5          200      0.8081303  0.9765309  0.18299177
  0.5  5          250      0.8055756  0.9743738  0.18751164
  0.7  1           50      0.8234721  0.9886297  0.13902949
  0.7  1          100      0.8248649  0.9884284  0.14178118
  0.7  1          150      0.8254962  0.9881696  0.14479513
  0.7  1          200      0.8257605  0.9881120  0.14427080
  0.7  1          250      0.8258527  0.9881312  0.14466433
  0.7  2           50      0.8241887  0.9860892  0.15501605
  0.7  2          100      0.8239088  0.9856769  0.15449184
  0.7  2          150      0.8231950  0.9851209  0.15757085
  0.7  2          200      0.8224392  0.9845169  0.16012595
  0.7  2          250      0.8217963  0.9839225  0.15999518
  0.7  3           50      0.8229773  0.9853893  0.15606381
  0.7  3          100      0.8205464  0.9837403  0.16091171
  0.7  3          150      0.8182073  0.9825611  0.16399063
  0.7  3          200      0.8163374  0.9811901  0.16765971
  0.7  3          250      0.8142436  0.9799726  0.17034558
  0.7  4           50      0.8202558  0.9835677  0.16025645
  0.7  4          100      0.8150684  0.9805095  0.16667727
  0.7  4          150      0.8106491  0.9782277  0.17329476
  0.7  4          200      0.8066228  0.9757063  0.18102589
  0.7  4          250      0.8032746  0.9738752  0.18711897
  0.7  5           50      0.8145889  0.9808354  0.16825009
  0.7  5          100      0.8070542  0.9761569  0.18194436
  0.7  5          150      0.8015318  0.9723508  0.19242679
  0.7  5          200      0.7969925  0.9693788  0.19766741
  0.7  5          250      0.7934850  0.9656399  0.20690502

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 1

Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using  the largest value.
The final values used for the model were nrounds = 200, max_depth = 4, eta
 = 0.1, gamma = 0, colsample_bytree = 1 and min_child_weight = 1. 

Elapsed time
Time difference of 40.5282 mins

Highest ROC value:
   TrainROC TrainSens TrainSpec  method
1 0.8270333 0.9873259 0.1480707 xgbTree

Confusion Matrix:

       reference
data    False  True
  False 44209  5569
  True    494   971

F-measure = 0.935828367608302

G-mean = 0.383184339781905

Matthews phi = NA

Balance = 0.397827331811919

===============================

Seed:
122
xgbTree_run# 7
eXtreme Gradient Boosting 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 107613, 107613, 107614, 107613, 107613, 107612, ... 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  ROC        Sens       Spec      
  0.1  1           50      0.8152478  1.0000000  0.00000000
  0.1  1          100      0.8226541  0.9957050  0.08104645
  0.1  1          150      0.8249285  0.9932795  0.10594318
  0.1  1          200      0.8258709  0.9914579  0.12101273
  0.1  1          250      0.8263194  0.9903458  0.12913696
  0.1  2           50      0.8222286  0.9940848  0.09736006
  0.1  2          100      0.8261874  0.9888694  0.14047130
  0.1  2          150      0.8270507  0.9873738  0.15226371
  0.1  2          200      0.8272547  0.9867698  0.15645704
  0.1  2          250      0.8274532  0.9863959  0.15914290
  0.1  3           50      0.8250694  0.9917360  0.11839167
  0.1  3          100      0.8273186  0.9872780  0.15206755
  0.1  3          150      0.8276870  0.9866164  0.15822615
  0.1  3          200      0.8279056  0.9865302  0.16012599
  0.1  3          250      0.8278865  0.9861371  0.16209140
  0.1  4           50      0.8264757  0.9902404  0.13221575
  0.1  4          100      0.8277295  0.9875368  0.15396764
  0.1  4          150      0.8278627  0.9867986  0.15632593
  0.1  4          200      0.8278066  0.9864535  0.15816067
  0.1  4          250      0.8277130  0.9861946  0.16071577
  0.1  5           50      0.8271699  0.9887352  0.14296061
  0.1  5          100      0.8277510  0.9870095  0.15704656
  0.1  5          150      0.8275830  0.9863768  0.15848789
  0.1  5          200      0.8272921  0.9858782  0.16058453
  0.1  5          250      0.8268718  0.9855810  0.16241948
  0.3  1           50      0.8245704  0.9914004  0.11924297
  0.3  1          100      0.8263671  0.9886681  0.14230577
  0.3  1          150      0.8267409  0.9882271  0.14754751
  0.3  1          200      0.8269703  0.9879874  0.14905429
  0.3  1          250      0.8270756  0.9877957  0.15016810
  0.3  2           50      0.8262249  0.9866260  0.15416449
  0.3  2          100      0.8270648  0.9858878  0.16130537
  0.3  2          150      0.8271480  0.9855235  0.16294304
  0.3  2          200      0.8273288  0.9855235  0.16261534
  0.3  2          250      0.8273401  0.9853797  0.16353308
  0.3  3           50      0.8270852  0.9860987  0.16137125
  0.3  3          100      0.8270631  0.9852359  0.16464701
  0.3  3          150      0.8265398  0.9847086  0.16726786
  0.3  3          200      0.8260371  0.9842197  0.16720220
  0.3  3          250      0.8255832  0.9841430  0.16818469
  0.3  4           50      0.8270434  0.9863001  0.16006093
  0.3  4          100      0.8260437  0.9852934  0.16320576
  0.3  4          150      0.8246453  0.9846319  0.16589201
  0.3  4          200      0.8232949  0.9840088  0.16844703
  0.3  4          250      0.8218345  0.9826666  0.16785764
  0.3  5           50      0.8261993  0.9853126  0.16399183
  0.3  5          100      0.8238233  0.9837595  0.16759543
  0.3  5          150      0.8213517  0.9826474  0.16929833
  0.3  5          200      0.8191934  0.9816311  0.17270528
  0.3  5          250      0.8173489  0.9807108  0.17657092
  0.5  1           50      0.8243135  0.9884476  0.14237165
  0.5  1          100      0.8258922  0.9883613  0.14617153
  0.5  1          150      0.8262945  0.9882558  0.14735049
  0.5  1          200      0.8265275  0.9880449  0.14879229
  0.5  1          250      0.8266214  0.9879395  0.15003695
  0.5  2           50      0.8262133  0.9858207  0.16261629
  0.5  2          100      0.8263619  0.9855235  0.16327130
  0.5  2          150      0.8260877  0.9850538  0.16543274
  0.5  2          200      0.8259989  0.9849866  0.16549866
  0.5  2          250      0.8259426  0.9848332  0.16831592
  0.5  3           50      0.8258018  0.9846415  0.16818422
  0.5  3          100      0.8247948  0.9839896  0.16942952
  0.5  3          150      0.8233391  0.9829158  0.17165671
  0.5  3          200      0.8220133  0.9823214  0.17342634
  0.5  3          250      0.8207345  0.9816695  0.17460499
  0.5  4           50      0.8244655  0.9843827  0.16707084
  0.5  4          100      0.8221330  0.9828583  0.17316374
  0.5  4          150      0.8191972  0.9810751  0.17886394
  0.5  4          200      0.8165440  0.9798384  0.18122219
  0.5  4          250      0.8143296  0.9784866  0.18377733
  0.5  5           50      0.8225430  0.9829830  0.17224640
  0.5  5          100      0.8173962  0.9801452  0.18141934
  0.5  5          150      0.8126129  0.9776333  0.18744744
  0.5  5          200      0.8089062  0.9759652  0.19229560
  0.5  5          250      0.8056655  0.9738368  0.19393370
  0.7  1           50      0.8244319  0.9878724  0.14761218
  0.7  1          100      0.8261111  0.9877957  0.14944700
  0.7  1          150      0.8264399  0.9876135  0.15115084
  0.7  1          200      0.8266201  0.9875943  0.15108514
  0.7  1          250      0.8267522  0.9874122  0.15167488
  0.7  2           50      0.8252151  0.9854085  0.16549827
  0.7  2          100      0.8251415  0.9849483  0.16700509
  0.7  2          150      0.8244524  0.9848524  0.16536699
  0.7  2          200      0.8238906  0.9843251  0.16628494
  0.7  2          250      0.8233772  0.9835006  0.17067439
  0.7  3           50      0.8235078  0.9838937  0.16825091
  0.7  3          100      0.8210517  0.9826282  0.17172348
  0.7  3          150      0.8180397  0.9812668  0.17493355
  0.7  3          200      0.8153732  0.9798767  0.18043779
  0.7  3          250      0.8136140  0.9789084  0.18155100
  0.7  4           50      0.8211499  0.9817654  0.17565374
  0.7  4          100      0.8157840  0.9791577  0.18266472
  0.7  4          150      0.8114478  0.9766555  0.18738230
  0.7  4          200      0.8075255  0.9744984  0.19275535
  0.7  4          250      0.8043035  0.9729165  0.19655481
  0.7  5           50      0.8180219  0.9800014  0.17906061
  0.7  5          100      0.8103760  0.9754092  0.19347516
  0.7  5          150      0.8044297  0.9725426  0.19871685
  0.7  5          200      0.7989491  0.9688707  0.20566140
  0.7  5          250      0.7946116  0.9665890  0.20946120

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 1

Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using  the largest value.
The final values used for the model were nrounds = 200, max_depth = 3, eta
 = 0.1, gamma = 0, colsample_bytree = 1 and min_child_weight = 1. 

Elapsed time
Time difference of 40.0068 mins

Highest ROC value:
   TrainROC TrainSens TrainSpec  method
1 0.8279056 0.9865302  0.160126 xgbTree

Confusion Matrix:

       reference
data    False  True
  False 44113  5537
  True    590  1003

F-measure = 0.935063008065456

G-mean = 0.389024142939042

Matthews phi = NA

Balance = 0.401265144458791

===============================

Seed:
156
xgbTree_run# 8
eXtreme Gradient Boosting 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 107613, 107613, 107612, 107613, 107612, 107614, ... 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  ROC        Sens       Spec      
  0.1  1           50      0.8153748  1.0000000  0.00000000
  0.1  1          100      0.8223720  0.9957338  0.08235591
  0.1  1          150      0.8249399  0.9933561  0.10659728
  0.1  1          200      0.8259794  0.9914675  0.12225580
  0.1  1          250      0.8264101  0.9903841  0.13096955
  0.1  2           50      0.8221290  0.9942861  0.09421505
  0.1  2          100      0.8262264  0.9888502  0.14125595
  0.1  2          150      0.8270354  0.9872300  0.15291769
  0.1  2          200      0.8274452  0.9865972  0.15770062
  0.1  2          250      0.8277103  0.9862042  0.15999394
  0.1  3           50      0.8250956  0.9921482  0.11485322
  0.1  3          100      0.8274992  0.9874505  0.15357282
  0.1  3          150      0.8279309  0.9866835  0.15835558
  0.1  3          200      0.8281774  0.9864534  0.16058346
  0.1  3          250      0.8283031  0.9862521  0.16281108
  0.1  4           50      0.8266619  0.9903075  0.13024879
  0.1  4          100      0.8279450  0.9875751  0.15304888
  0.1  4          150      0.8282257  0.9869711  0.15711097
  0.1  4          200      0.8282565  0.9866356  0.15868332
  0.1  4          250      0.8280979  0.9862905  0.15979688
  0.1  5           50      0.8274562  0.9892625  0.13811060
  0.1  5          100      0.8280765  0.9870670  0.15370401
  0.1  5          150      0.8280930  0.9865301  0.15789726
  0.1  5          200      0.8278273  0.9861275  0.16058359
  0.1  5          250      0.8274669  0.9859453  0.16241815
  0.3  1           50      0.8243742  0.9920715  0.11648999
  0.3  1          100      0.8264173  0.9887352  0.14060051
  0.3  1          150      0.8269047  0.9882175  0.14610394
  0.3  1          200      0.8270620  0.9879299  0.14682457
  0.3  1          250      0.8271606  0.9878532  0.14839683
  0.3  2           50      0.8265420  0.9862617  0.15796283
  0.3  2          100      0.8274042  0.9855714  0.16418740
  0.3  2          150      0.8276130  0.9856386  0.16497351
  0.3  2          200      0.8277152  0.9858015  0.16392523
  0.3  2          250      0.8275856  0.9857152  0.16313942
  0.3  3           50      0.8274640  0.9860987  0.16025576
  0.3  3          100      0.8276032  0.9855331  0.16399033
  0.3  3          150      0.8272470  0.9849962  0.16713487
  0.3  3          200      0.8265843  0.9849003  0.16680768
  0.3  3          250      0.8257623  0.9846223  0.16870756
  0.3  4           50      0.8273782  0.9862809  0.16071435
  0.3  4          100      0.8265417  0.9850058  0.16510427
  0.3  4          150      0.8253015  0.9843251  0.16516967
  0.3  4          200      0.8238888  0.9835102  0.16811791
  0.3  4          250      0.8224385  0.9829445  0.17100058
  0.3  5           50      0.8266030  0.9857919  0.16117328
  0.3  5          100      0.8244659  0.9842101  0.17001844
  0.3  5          150      0.8223733  0.9831267  0.17205015
  0.3  5          200      0.8201494  0.9820242  0.17624322
  0.3  5          250      0.8183982  0.9807971  0.17919198
  0.5  1           50      0.8244282  0.9876614  0.14813514
  0.5  1          100      0.8258829  0.9877765  0.14878993
  0.5  1          150      0.8263877  0.9876806  0.15049322
  0.5  1          200      0.8266005  0.9876039  0.15101729
  0.5  1          250      0.8266987  0.9875847  0.15154145
  0.5  2           50      0.8264826  0.9854180  0.16608758
  0.5  2          100      0.8269270  0.9856481  0.16412208
  0.5  2          150      0.8265186  0.9856481  0.16412139
  0.5  2          200      0.8261502  0.9852742  0.16379456
  0.5  2          250      0.8256921  0.9847757  0.16458075
  0.5  3           50      0.8264361  0.9856769  0.16379391
  0.5  3          100      0.8252336  0.9849674  0.16824936
  0.5  3          150      0.8238936  0.9839896  0.17106629
  0.5  3          200      0.8223941  0.9833952  0.17086969
  0.5  3          250      0.8212373  0.9826378  0.17421125
  0.5  4           50      0.8246295  0.9846511  0.16713568
  0.5  4          100      0.8218812  0.9828103  0.17414640
  0.5  4          150      0.8198331  0.9809888  0.17801195
  0.5  4          200      0.8176451  0.9798000  0.18299070
  0.5  4          250      0.8146041  0.9785153  0.18561133
  0.5  5           50      0.8223995  0.9827624  0.17303195
  0.5  5          100      0.8174933  0.9801931  0.18069669
  0.5  5          150      0.8134153  0.9779880  0.18633131
  0.5  5          200      0.8097287  0.9759748  0.19111450
  0.5  5          250      0.8061661  0.9740094  0.19517681
  0.7  1           50      0.8245164  0.9881504  0.14472870
  0.7  1          100      0.8261584  0.9877669  0.14970719
  0.7  1          150      0.8265521  0.9876710  0.15088662
  0.7  1          200      0.8266561  0.9876135  0.15239361
  0.7  1          250      0.8268029  0.9877477  0.15180426
  0.7  2           50      0.8256302  0.9856961  0.16300750
  0.7  2          100      0.8254530  0.9853318  0.16320383
  0.7  2          150      0.8244117  0.9847565  0.16575988
  0.7  2          200      0.8235808  0.9840087  0.16687429
  0.7  2          250      0.8227276  0.9835581  0.16910165
  0.7  3           50      0.8242138  0.9846798  0.16621816
  0.7  3          100      0.8215701  0.9833280  0.17139428
  0.7  3          150      0.8193740  0.9821872  0.17532596
  0.7  3          200      0.8177253  0.9809313  0.17971519
  0.7  3          250      0.8155021  0.9797329  0.18318768
  0.7  4           50      0.8210836  0.9822639  0.17611143
  0.7  4          100      0.8161713  0.9793973  0.18181183
  0.7  4          150      0.8122735  0.9773265  0.18882243
  0.7  4          200      0.8090174  0.9751023  0.19635761
  0.7  4          250      0.8056673  0.9732137  0.19832281
  0.7  5           50      0.8175459  0.9798671  0.17820820
  0.7  5          100      0.8104210  0.9755337  0.19013347
  0.7  5          150      0.8046051  0.9720440  0.19845391
  0.7  5          200      0.8006930  0.9695131  0.20749536
  0.7  5          250      0.7967920  0.9667807  0.21273658

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 1

Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using  the largest value.
The final values used for the model were nrounds = 250, max_depth = 3, eta
 = 0.1, gamma = 0, colsample_bytree = 1 and min_child_weight = 1. 

Elapsed time
Time difference of 40.31084 mins

Highest ROC value:
   TrainROC TrainSens TrainSpec  method
1 0.8283031 0.9862521 0.1628111 xgbTree

Confusion Matrix:

       reference
data    False  True
  False 44102  5505
  True    601  1035

F-measure = 0.935256070406107

G-mean = 0.395131914269475

Matthews phi = NA

Balance = 0.404721818093554

===============================

Seed:
233
xgbTree_run# 9
eXtreme Gradient Boosting 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 107612, 107613, 107612, 107613, 107614, 107613, ... 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  ROC        Sens       Spec      
  0.1  1           50      0.8139343  1.0000000  0.00000000
  0.1  1          100      0.8215278  0.9957721  0.08170146
  0.1  1          150      0.8239350  0.9934904  0.10292880
  0.1  1          200      0.8249455  0.9918797  0.11786686
  0.1  1          250      0.8254094  0.9908156  0.12677734
  0.1  2           50      0.8211584  0.9948613  0.09015300
  0.1  2          100      0.8251724  0.9891378  0.13902949
  0.1  2          150      0.8260273  0.9873450  0.15036430
  0.1  2          200      0.8263997  0.9868082  0.15534340
  0.1  2          250      0.8266621  0.9865301  0.15743991
  0.1  3           50      0.8241664  0.9923303  0.11334635
  0.1  3          100      0.8263330  0.9874697  0.15036396
  0.1  3          150      0.8267172  0.9868561  0.15567071
  0.1  3          200      0.8268426  0.9865206  0.15809501
  0.1  3          250      0.8268781  0.9863959  0.15947094
  0.1  4           50      0.8254573  0.9906526  0.12638497
  0.1  4          100      0.8266064  0.9880641  0.14977440
  0.1  4          150      0.8267469  0.9872779  0.15416423
  0.1  4          200      0.8266932  0.9868465  0.15671916
  0.1  4          250      0.8265557  0.9865589  0.15809505
  0.1  5           50      0.8263127  0.9892145  0.13654014
  0.1  5          100      0.8268611  0.9871917  0.15291902
  0.1  5          150      0.8266270  0.9868849  0.15619431
  0.1  5          200      0.8262849  0.9863192  0.15933936
  0.1  5          250      0.8255973  0.9858591  0.16091180
  0.3  1           50      0.8234947  0.9919085  0.11622945
  0.3  1          100      0.8252625  0.9886681  0.14138804
  0.3  1          150      0.8255975  0.9881504  0.14531895
  0.3  1          200      0.8257202  0.9880353  0.14617050
  0.3  1          250      0.8258096  0.9879970  0.14728418
  0.3  2           50      0.8254586  0.9868273  0.15422985
  0.3  2          100      0.8265188  0.9862425  0.15927439
  0.3  2          150      0.8266649  0.9858111  0.16248480
  0.3  2          200      0.8266007  0.9859741  0.16124010
  0.3  2          250      0.8265114  0.9857632  0.16169860
  0.3  3           50      0.8260969  0.9864439  0.15698171
  0.3  3          100      0.8261377  0.9859741  0.16189511
  0.3  3          150      0.8256640  0.9855139  0.16503960
  0.3  3          200      0.8248332  0.9850346  0.16464667
  0.3  3          250      0.8242583  0.9847853  0.16543291
  0.3  4           50      0.8256097  0.9859549  0.15907797
  0.3  4          100      0.8244958  0.9855235  0.16209226
  0.3  4          150      0.8233810  0.9846031  0.16458157
  0.3  4          200      0.8219622  0.9839320  0.16543304
  0.3  4          250      0.8205224  0.9831555  0.16746484
  0.3  5           50      0.8250485  0.9858207  0.15809548
  0.3  5          100      0.8221913  0.9843059  0.16431919
  0.3  5          150      0.8196768  0.9830500  0.16766014
  0.3  5          200      0.8177338  0.9821105  0.17106744
  0.3  5          250      0.8163454  0.9813244  0.17316396
  0.5  1           50      0.8236974  0.9877765  0.14341851
  0.5  1          100      0.8248852  0.9884955  0.14355022
  0.5  1          150      0.8253313  0.9881504  0.14590842
  0.5  1          200      0.8255252  0.9881695  0.14643263
  0.5  1          250      0.8256949  0.9880545  0.14702240
  0.5  2           50      0.8255782  0.9861275  0.15796369
  0.5  2          100      0.8259695  0.9859262  0.16065015
  0.5  2          150      0.8256848  0.9859166  0.16143639
  0.5  2          200      0.8254604  0.9858399  0.16202621
  0.5  2          250      0.8251210  0.9855810  0.16281249
  0.5  3           50      0.8246538  0.9853030  0.16300776
  0.5  3          100      0.8236249  0.9848045  0.16621821
  0.5  3          150      0.8223415  0.9840854  0.16792222
  0.5  3          200      0.8209932  0.9831267  0.17001809
  0.5  3          250      0.8197296  0.9824748  0.17165636
  0.5  4           50      0.8235143  0.9844881  0.16687390
  0.5  4          100      0.8198774  0.9828966  0.17067417
  0.5  4          150      0.8169392  0.9816024  0.17427772
  0.5  4          200      0.8147019  0.9802410  0.17552302
  0.5  4          250      0.8124321  0.9786592  0.18109134
  0.5  5           50      0.8221172  0.9830884  0.17015053
  0.5  5          100      0.8167569  0.9802698  0.17539256
  0.5  5          150      0.8121590  0.9776525  0.18135385
  0.5  5          200      0.8079879  0.9757255  0.18626767
  0.5  5          250      0.8046538  0.9739423  0.19013309
  0.7  1           50      0.8233653  0.9871725  0.14748142
  0.7  1          100      0.8246497  0.9876135  0.14728461
  0.7  1          150      0.8250328  0.9874888  0.14852961
  0.7  1          200      0.8252974  0.9874697  0.14925028
  0.7  1          250      0.8255663  0.9876135  0.14944674
  0.7  2           50      0.8250134  0.9857440  0.15973280
  0.7  2          100      0.8251743  0.9852838  0.16150265
  0.7  2          150      0.8246009  0.9849291  0.16497445
  0.7  2          200      0.8238374  0.9845169  0.16648170
  0.7  2          250      0.8228629  0.9840087  0.16504080
  0.7  3           50      0.8223891  0.9844018  0.16457985
  0.7  3          100      0.8204729  0.9826090  0.16942866
  0.7  3          150      0.8176561  0.9810368  0.17368777
  0.7  3          200      0.8150034  0.9801547  0.17788080
  0.7  3          250      0.8137476  0.9788605  0.17978072
  0.7  4           50      0.8197535  0.9827528  0.16890488
  0.7  4          100      0.8150049  0.9798959  0.17663670
  0.7  4          150      0.8112042  0.9775854  0.18554632
  0.7  4          200      0.8074145  0.9755434  0.19059077
  0.7  4          250      0.8040840  0.9739615  0.19452279
  0.7  5           50      0.8149392  0.9793398  0.17585012
  0.7  5          100      0.8077467  0.9751503  0.18705413
  0.7  5          150      0.8010695  0.9714976  0.19426157
  0.7  5          200      0.7964727  0.9685256  0.20140327
  0.7  5          250      0.7934012  0.9658412  0.20775800

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 1

Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using  the largest value.
The final values used for the model were nrounds = 250, max_depth = 3, eta
 = 0.1, gamma = 0, colsample_bytree = 1 and min_child_weight = 1. 

Elapsed time
Time difference of 40.0568 mins

Highest ROC value:
   TrainROC TrainSens TrainSpec  method
1 0.8268781 0.9863959 0.1594709 xgbTree

Confusion Matrix:

       reference
data    False  True
  False 44087  5490
  True    616  1050

F-measure = 0.935235468816292

G-mean = 0.397917202136381

Matthews phi = NA

Balance = 0.406339568340026

===============================

Seed:
454
xgbTree_run# 10
eXtreme Gradient Boosting 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 107613, 107613, 107612, 107613, 107614, 107612, ... 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  ROC        Sens       Spec      
  0.1  1           50      0.8142827  1.0000000  0.00000000
  0.1  1          100      0.8210694  0.9961076  0.07940711
  0.1  1          150      0.8236694  0.9938451  0.10161767
  0.1  1          200      0.8247173  0.9919852  0.11662169
  0.1  1          250      0.8251037  0.9907485  0.12533531
  0.1  2           50      0.8206306  0.9946216  0.09251082
  0.1  2          100      0.8247669  0.9893775  0.13503198
  0.1  2          150      0.8254440  0.9877669  0.14780813
  0.1  2          200      0.8258203  0.9872779  0.15154261
  0.1  2          250      0.8261414  0.9868369  0.15416363
  0.1  3           50      0.8234900  0.9928768  0.10875881
  0.1  3          100      0.8258782  0.9880833  0.14459772
  0.1  3          150      0.8264381  0.9872588  0.15036344
  0.1  3          200      0.8266800  0.9868465  0.15239451
  0.1  3          250      0.8266989  0.9865206  0.15377010
  0.1  4           50      0.8246925  0.9913333  0.12140463
  0.1  4          100      0.8260546  0.9881120  0.14617033
  0.1  4          150      0.8262154  0.9874122  0.15069088
  0.1  4          200      0.8260581  0.9870191  0.15344301
  0.1  4          250      0.8258971  0.9867027  0.15436001
  0.1  5           50      0.8256112  0.9898089  0.13116683
  0.1  5          100      0.8262587  0.9874889  0.14761141
  0.1  5          150      0.8260673  0.9867986  0.15416290
  0.1  5          200      0.8256612  0.9864631  0.15645592
  0.1  5          250      0.8252614  0.9860220  0.15848716
  0.3  1           50      0.8233250  0.9919468  0.11773584
  0.3  1          100      0.8248661  0.9890611  0.13804571
  0.3  1          150      0.8253134  0.9885147  0.14256661
  0.3  1          200      0.8255504  0.9882558  0.14427020
  0.3  1          250      0.8257116  0.9881312  0.14485967
  0.3  2           50      0.8246977  0.9864151  0.15422912
  0.3  2          100      0.8256153  0.9858015  0.16019100
  0.3  2          150      0.8261043  0.9855906  0.16137000
  0.3  2          200      0.8262266  0.9857249  0.16123890
  0.3  2          250      0.8259331  0.9855235  0.16189429
  0.3  3           50      0.8261073  0.9865302  0.15475306
  0.3  3          100      0.8261738  0.9855139  0.16163238
  0.3  3          150      0.8257139  0.9852647  0.16346699
  0.3  3          200      0.8249504  0.9850633  0.16222194
  0.3  3          250      0.8246795  0.9849004  0.16517049
  0.3  4           50      0.8253510  0.9867890  0.15455673
  0.3  4          100      0.8243304  0.9856577  0.15947012
  0.3  4          150      0.8230989  0.9848716  0.16254973
  0.3  4          200      0.8216829  0.9844402  0.16412195
  0.3  4          250      0.8202052  0.9837211  0.16752930
  0.3  5           50      0.8248392  0.9857440  0.15888116
  0.3  5          100      0.8224914  0.9843539  0.16543218
  0.3  5          150      0.8197614  0.9830596  0.16687317
  0.3  5          200      0.8176294  0.9820817  0.17067314
  0.3  5          250      0.8158574  0.9810942  0.17473588
  0.5  1           50      0.8231928  0.9881120  0.14256704
  0.5  1          100      0.8245539  0.9884763  0.14237023
  0.5  1          150      0.8251683  0.9882462  0.14322183
  0.5  1          200      0.8255225  0.9882079  0.14394263
  0.5  1          250      0.8256447  0.9881120  0.14420458
  0.5  2           50      0.8249814  0.9861467  0.15698077
  0.5  2          100      0.8253575  0.9861179  0.15861891
  0.5  2          150      0.8251079  0.9860508  0.15822547
  0.5  2          200      0.8246415  0.9858207  0.15796304
  0.5  2          250      0.8242558  0.9857632  0.16078040
  0.5  3           50      0.8244729  0.9854852  0.16300827
  0.5  3          100      0.8231577  0.9848045  0.16543205
  0.5  3          150      0.8220352  0.9841142  0.16752852
  0.5  3          200      0.8202354  0.9830884  0.16903522
  0.5  3          250      0.8189012  0.9824556  0.17139372
  0.5  4           50      0.8233064  0.9847278  0.16019040
  0.5  4          100      0.8200069  0.9828487  0.16693909
  0.5  4          150      0.8173679  0.9814682  0.17132910
  0.5  4          200      0.8150500  0.9802985  0.17617756
  0.5  4          250      0.8126769  0.9792535  0.17860177
  0.5  5           50      0.8211513  0.9830021  0.16608822
  0.5  5          100      0.8158934  0.9801739  0.17434227
  0.5  5          150      0.8110913  0.9781319  0.17958396
  0.5  5          200      0.8077355  0.9760994  0.18797010
  0.5  5          250      0.8041982  0.9742970  0.19032831
  0.7  1           50      0.8230180  0.9886393  0.13889774
  0.7  1          100      0.8245953  0.9885243  0.14309133
  0.7  1          150      0.8250900  0.9881791  0.14525303
  0.7  1          200      0.8253223  0.9882846  0.14440126
  0.7  1          250      0.8256126  0.9882654  0.14499104
  0.7  2           50      0.8242602  0.9856577  0.16176400
  0.7  2          100      0.8240780  0.9857824  0.15960071
  0.7  2          150      0.8232298  0.9852455  0.16019066
  0.7  2          200      0.8227102  0.9848237  0.16248368
  0.7  2          250      0.8217980  0.9846031  0.16444896
  0.7  3           50      0.8228562  0.9850058  0.15999531
  0.7  3          100      0.8202831  0.9837211  0.16530116
  0.7  3          150      0.8175188  0.9823406  0.16995278
  0.7  3          200      0.8151168  0.9813339  0.17466979
  0.7  3          250      0.8131612  0.9802027  0.17748706
  0.7  4           50      0.8194911  0.9823885  0.17001814
  0.7  4          100      0.8150710  0.9793207  0.17499715
  0.7  4          150      0.8103543  0.9768855  0.18207310
  0.7  4          200      0.8063020  0.9749681  0.18941152
  0.7  4          250      0.8035444  0.9734438  0.19308073
  0.7  5           50      0.8144139  0.9797904  0.17683175
  0.7  5          100      0.8070402  0.9754379  0.18377626
  0.7  5          150      0.8015332  0.9714113  0.19268703
  0.7  5          200      0.7975502  0.9684009  0.20382529
  0.7  5          250      0.7933000  0.9657645  0.20946017

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 1

Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using  the largest value.
The final values used for the model were nrounds = 250, max_depth = 3, eta
 = 0.1, gamma = 0, colsample_bytree = 1 and min_child_weight = 1. 

Elapsed time
Time difference of 40.44689 mins

Highest ROC value:
   TrainROC TrainSens TrainSpec  method
1 0.8266989 0.9865206 0.1537701 xgbTree

Confusion Matrix:

       reference
data    False  True
  False 44120  5493
  True    583  1047

F-measure = 0.935578268798507

G-mean = 0.397497026138459

Matthews phi = NA

Balance = 0.406023584602165
