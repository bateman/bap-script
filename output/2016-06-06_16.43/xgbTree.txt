
===============================

Seed:
849
xgbTree_run# 1
eXtreme Gradient Boosting 

No pre-processing
Resampling: Cross-Validated (3 fold, repeated 2 times) 
Summary of sample sizes: 233, 233, 234, 233, 234, 233, ... 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  ROC        Sens       Spec      
  0.1  1           50      0.7526572  1.0000000  0.00000000
  0.1  1          100      0.7558824  0.9967320  0.01190476
  0.1  1          150      0.7580065  0.9869281  0.04523810
  0.1  1          200      0.7539994  0.9754902  0.05714286
  0.1  1          250      0.7500000  0.9673203  0.07857143
  0.1  2           50      0.7407330  0.9787582  0.03412698
  0.1  2          100      0.7317849  0.9738562  0.10238095
  0.1  2          150      0.7268830  0.9656863  0.13650794
  0.1  2          200      0.7228447  0.9591503  0.12539683
  0.1  2          250      0.7189698  0.9542484  0.12539683
  0.1  3           50      0.7333567  0.9722222  0.06825397
  0.1  3          100      0.7322051  0.9575163  0.11428571
  0.1  3          150      0.7252412  0.9526144  0.11428571
  0.1  3          200      0.7203315  0.9542484  0.11428571
  0.1  3          250      0.7186119  0.9575163  0.11428571
  0.1  4           50      0.7196623  0.9722222  0.15952381
  0.1  4          100      0.7137255  0.9640523  0.13650794
  0.1  4          150      0.7121849  0.9607843  0.12539683
  0.1  4          200      0.7071973  0.9607843  0.12539683
  0.1  4          250      0.7056489  0.9607843  0.12539683
  0.1  5           50      0.7234205  0.9787582  0.11428571
  0.1  5          100      0.7111111  0.9705882  0.13650794
  0.1  5          150      0.7096327  0.9640523  0.12539683
  0.1  5          200      0.7098506  0.9640523  0.12539683
  0.1  5          250      0.7096639  0.9607843  0.12539683
  0.3  1           50      0.7569250  0.9836601  0.05634921
  0.3  1          100      0.7478758  0.9656863  0.10079365
  0.3  1          150      0.7381497  0.9526144  0.11269841
  0.3  1          200      0.7341192  0.9477124  0.10158730
  0.3  1          250      0.7268674  0.9460784  0.11349206
  0.3  2           50      0.7289293  0.9591503  0.12539683
  0.3  2          100      0.7150560  0.9477124  0.12539683
  0.3  2          150      0.7211173  0.9477124  0.12539683
  0.3  2          200      0.7216075  0.9493464  0.11349206
  0.3  2          250      0.7245020  0.9477124  0.11349206
  0.3  3           50      0.7202770  0.9624183  0.15952381
  0.3  3          100      0.7162621  0.9542484  0.15873016
  0.3  3          150      0.7159819  0.9477124  0.14761905
  0.3  3          200      0.7187053  0.9477124  0.14761905
  0.3  3          250      0.7213819  0.9444444  0.13650794
  0.3  4           50      0.7078587  0.9591503  0.12539683
  0.3  4          100      0.7064426  0.9526144  0.13650794
  0.3  4          150      0.7083022  0.9526144  0.13650794
  0.3  4          200      0.7094538  0.9558824  0.13650794
  0.3  4          250      0.7132820  0.9542484  0.15952381
  0.3  5           50      0.7220433  0.9591503  0.12539683
  0.3  5          100      0.7238251  0.9607843  0.11349206
  0.3  5          150      0.7255447  0.9575163  0.13650794
  0.3  5          200      0.7271786  0.9575163  0.13650794
  0.3  5          250      0.7280657  0.9575163  0.13650794
  0.5  1           50      0.7421024  0.9656863  0.11349206
  0.5  1          100      0.7342982  0.9411765  0.11269841
  0.5  1          150      0.7299175  0.9444444  0.12460317
  0.5  1          200      0.7158886  0.9362745  0.13571429
  0.5  1          250      0.7154684  0.9281046  0.13571429
  0.5  2           50      0.7106754  0.9542484  0.14761905
  0.5  2          100      0.7104964  0.9493464  0.12460317
  0.5  2          150      0.7112356  0.9477124  0.13650794
  0.5  2          200      0.7145503  0.9428105  0.13650794
  0.5  2          250      0.7156240  0.9460784  0.14761905
  0.5  3           50      0.7130641  0.9542484  0.09047619
  0.5  3          100      0.7126595  0.9428105  0.10238095
  0.5  3          150      0.7153984  0.9444444  0.12460317
  0.5  3          200      0.7187753  0.9444444  0.13650794
  0.5  3          250      0.7186041  0.9428105  0.13650794
  0.5  4           50      0.7039449  0.9526144  0.13650794
  0.5  4          100      0.7053999  0.9558824  0.14761905
  0.5  4          150      0.7102863  0.9558824  0.15873016
  0.5  4          200      0.7137411  0.9542484  0.15873016
  0.5  4          250      0.7133053  0.9542484  0.15873016
  0.5  5           50      0.7238095  0.9607843  0.14761905
  0.5  5          100      0.7209228  0.9591503  0.16984127
  0.5  5          150      0.7258248  0.9575163  0.16984127
  0.5  5          200      0.7264395  0.9575163  0.18095238
  0.5  5          250      0.7261983  0.9591503  0.18095238
  0.7  1           50      0.7313375  0.9509804  0.10158730
  0.7  1          100      0.7186041  0.9428105  0.10158730
  0.7  1          150      0.7147214  0.9330065  0.13571429
  0.7  1          200      0.7092204  0.9281046  0.14682540
  0.7  1          250      0.7023459  0.9264706  0.14682540
  0.7  2           50      0.7056567  0.9379085  0.14603175
  0.7  2          100      0.7025132  0.9395425  0.16904762
  0.7  2          150      0.7048708  0.9411765  0.15793651
  0.7  2          200      0.7058279  0.9411765  0.16984127
  0.7  2          250      0.7040461  0.9411765  0.16984127
  0.7  3           50      0.7059757  0.9558824  0.16904762
  0.7  3          100      0.7133754  0.9477124  0.18095238
  0.7  3          150      0.7129474  0.9509804  0.16984127
  0.7  3          200      0.7127840  0.9542484  0.16984127
  0.7  3          250      0.7115663  0.9558824  0.15873016
  0.7  4           50      0.7045363  0.9493464  0.15793651
  0.7  4          100      0.7080766  0.9477124  0.18174603
  0.7  4          150      0.7089013  0.9428105  0.20396825
  0.7  4          200      0.7058746  0.9428105  0.21507937
  0.7  4          250      0.7036103  0.9395425  0.20396825
  0.7  5           50      0.7129474  0.9542484  0.18253968
  0.7  5          100      0.7136166  0.9526144  0.17142857
  0.7  5          150      0.7169623  0.9542484  0.19365079
  0.7  5          200      0.7145425  0.9526144  0.17142857
  0.7  5          250      0.7132509  0.9542484  0.14841270

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 1

Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using  the largest value.
The final values used for the model were nrounds = 150, max_depth = 1, eta
 = 0.1, gamma = 0, colsample_bytree = 1 and min_child_weight = 1. 

Elapsed time
Time difference of 7.732086 secs

Highest ROC value:
   TrainROC TrainSens TrainSpec  method
1 0.7580065 0.9869281 0.0452381 xgbTree

Confusion Matrix:

       reference
data    False True
  False   128   17
  True      2    1

F-measure = 0.930909090909091

G-mean = 0.233882138481874

Matthews phi = 0.0931698285738416

Balance = 0.332088331052486

===============================

Seed:
342
xgbTree_run# 2
eXtreme Gradient Boosting 

No pre-processing
Resampling: Cross-Validated (3 fold, repeated 2 times) 
Summary of sample sizes: 233, 233, 234, 233, 233, 234, ... 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  ROC        Sens       Spec      
  0.1  1           50      0.7738095  0.9967320  0.01111111
  0.1  1          100      0.7681334  0.9950980  0.08888889
  0.1  1          150      0.7657524  0.9918301  0.10079365
  0.1  1          200      0.7622316  0.9820261  0.11269841
  0.1  1          250      0.7603097  0.9771242  0.12460317
  0.1  2           50      0.7442810  0.9852941  0.08888889
  0.1  2          100      0.7445378  0.9722222  0.14523810
  0.1  2          150      0.7409353  0.9656863  0.16825397
  0.1  2          200      0.7407330  0.9526144  0.19126984
  0.1  2          250      0.7390678  0.9526144  0.21428571
  0.1  3           50      0.7376050  0.9738562  0.11190476
  0.1  3          100      0.7291628  0.9673203  0.18095238
  0.1  3          150      0.7290305  0.9656863  0.16984127
  0.1  3          200      0.7296841  0.9656863  0.15793651
  0.1  3          250      0.7316215  0.9656863  0.15793651
  0.1  4           50      0.7341192  0.9689542  0.15793651
  0.1  4          100      0.7240585  0.9689542  0.13492063
  0.1  4          150      0.7265717  0.9640523  0.15793651
  0.1  4          200      0.7306956  0.9624183  0.15793651
  0.1  4          250      0.7317927  0.9624183  0.16904762
  0.1  5           50      0.7367258  0.9640523  0.16904762
  0.1  5          100      0.7277856  0.9640523  0.13492063
  0.1  5          150      0.7328431  0.9673203  0.15714286
  0.1  5          200      0.7364690  0.9640523  0.14603175
  0.1  5          250      0.7367803  0.9656863  0.15793651
  0.3  1           50      0.7601035  0.9869281  0.10079365
  0.3  1          100      0.7539255  0.9689542  0.13571429
  0.3  1          150      0.7468215  0.9624183  0.14761905
  0.3  1          200      0.7399082  0.9477124  0.19365079
  0.3  1          250      0.7370526  0.9444444  0.18253968
  0.3  2           50      0.7309913  0.9542484  0.18015873
  0.3  2          100      0.7311313  0.9460784  0.20317460
  0.3  2          150      0.7351930  0.9526144  0.22619048
  0.3  2          200      0.7391145  0.9526144  0.19285714
  0.3  2          250      0.7415811  0.9558824  0.20396825
  0.3  3           50      0.7282291  0.9591503  0.15793651
  0.3  3          100      0.7286337  0.9558824  0.15793651
  0.3  3          150      0.7272176  0.9575163  0.16904762
  0.3  3          200      0.7277233  0.9591503  0.16904762
  0.3  3          250      0.7278322  0.9558824  0.16904762
  0.3  4           50      0.7324463  0.9591503  0.15793651
  0.3  4          100      0.7343215  0.9575163  0.14682540
  0.3  4          150      0.7355742  0.9575163  0.15793651
  0.3  4          200      0.7384765  0.9575163  0.14682540
  0.3  4          250      0.7396125  0.9624183  0.14682540
  0.3  5           50      0.7245643  0.9689542  0.13492063
  0.3  5          100      0.7307345  0.9673203  0.14603175
  0.3  5          150      0.7317616  0.9607843  0.16825397
  0.3  5          200      0.7312481  0.9624183  0.14603175
  0.3  5          250      0.7336601  0.9607843  0.15714286
  0.5  1           50      0.7538282  0.9673203  0.12460317
  0.5  1          100      0.7404801  0.9526144  0.13730159
  0.5  1          150      0.7374961  0.9477124  0.17063492
  0.5  1          200      0.7299175  0.9411765  0.18174603
  0.5  1          250      0.7278245  0.9395425  0.15793651
  0.5  2           50      0.7413321  0.9444444  0.19206349
  0.5  2          100      0.7356676  0.9477124  0.18015873
  0.5  2          150      0.7387488  0.9428105  0.18015873
  0.5  2          200      0.7415033  0.9444444  0.18015873
  0.5  2          250      0.7420090  0.9411765  0.19126984
  0.5  3           50      0.7200747  0.9542484  0.15714286
  0.5  3          100      0.7229381  0.9542484  0.15714286
  0.5  3          150      0.7288282  0.9558824  0.15714286
  0.5  3          200      0.7323996  0.9542484  0.15714286
  0.5  3          250      0.7318083  0.9542484  0.15714286
  0.5  4           50      0.7259960  0.9673203  0.15793651
  0.5  4          100      0.7301743  0.9673203  0.15793651
  0.5  4          150      0.7326097  0.9656863  0.16904762
  0.5  4          200      0.7347728  0.9656863  0.16984127
  0.5  4          250      0.7345005  0.9656863  0.18095238
  0.5  5           50      0.7295518  0.9591503  0.13412698
  0.5  5          100      0.7329132  0.9575163  0.14523810
  0.5  5          150      0.7368114  0.9575163  0.14523810
  0.5  5          200      0.7384065  0.9591503  0.13412698
  0.5  5          250      0.7396981  0.9591503  0.14523810
  0.7  1           50      0.7349245  0.9575163  0.11269841
  0.7  1          100      0.7295518  0.9428105  0.15952381
  0.7  1          150      0.7264706  0.9379085  0.18253968
  0.7  1          200      0.7294507  0.9313725  0.16984127
  0.7  1          250      0.7264862  0.9330065  0.15793651
  0.7  2           50      0.7306217  0.9509804  0.20317460
  0.7  2          100      0.7380174  0.9509804  0.19206349
  0.7  2          150      0.7409819  0.9526144  0.20317460
  0.7  2          200      0.7417834  0.9493464  0.20317460
  0.7  2          250      0.7448179  0.9509804  0.18015873
  0.7  3           50      0.7139589  0.9477124  0.16984127
  0.7  3          100      0.7265251  0.9509804  0.18095238
  0.7  3          150      0.7269141  0.9477124  0.19285714
  0.7  3          200      0.7270230  0.9460784  0.20396825
  0.7  3          250      0.7282913  0.9477124  0.21587302
  0.7  4           50      0.7227980  0.9542484  0.14682540
  0.7  4          100      0.7343137  0.9509804  0.15793651
  0.7  4          150      0.7355742  0.9509804  0.14682540
  0.7  4          200      0.7357843  0.9526144  0.15873016
  0.7  4          250      0.7374572  0.9526144  0.15873016
  0.7  5           50      0.7377529  0.9575163  0.18095238
  0.7  5          100      0.7408419  0.9575163  0.18095238
  0.7  5          150      0.7448179  0.9575163  0.19126984
  0.7  5          200      0.7462652  0.9591503  0.20317460
  0.7  5          250      0.7461951  0.9591503  0.20317460

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'colsample_bytree' was held constant at a value of 1

Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using  the largest value.
The final values used for the model were nrounds = 50, max_depth = 1, eta
 = 0.1, gamma = 0, colsample_bytree = 1 and min_child_weight = 1. 

Elapsed time
Time difference of 7.820491 secs

Highest ROC value:
   TrainROC TrainSens  TrainSpec  method
1 0.7738095  0.996732 0.01111111 xgbTree

Confusion Matrix:

       reference
data    False True
  False   130   18
  True      0    0

F-measure = 0.935251798561151

G-mean = 0

Matthews phi = NaN

Balance = 0.292893218813453
